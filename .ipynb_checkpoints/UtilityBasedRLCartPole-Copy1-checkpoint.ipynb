{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Greedy Utility-based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import operator\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env._max_episode_steps = 5000\n",
    "ACTION_SPACE = env.action_space.n # number of possible actions\n",
    "OBSERVATION_SPACE = env.observation_space.shape[0] # number of observable variables\n",
    "EXPLORATION_PROB = 1.0 # EPSILON GREEDY STRATEGY\n",
    "EXPLORATION_DECAY = 0.9995\n",
    "EXPLORATION_STOP = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_intervals():\n",
    "    intervals = np.zeros((OBSERVATION_SPACE, STATES_IN_INTERVAL))\n",
    "    intervals[0] = np.linspace(-4.8, 4.8, STATES_IN_INTERVAL)\n",
    "    intervals[1] = np.linspace(-3.5, 3.5, STATES_IN_INTERVAL)\n",
    "    intervals[2] = np.linspace(-0.42, 0.42, STATES_IN_INTERVAL)\n",
    "    intervals[3] = np.linspace(-4, 4, STATES_IN_INTERVAL)\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_observation(observation):\n",
    "    discrete_observation = np.array([np.digitize(observation[index], INTERVALS[index])-1 for index in range(OBSERVATION_SPACE)])\n",
    "    # if some value is under the lower border ignore it and give it min value\n",
    "    discrete_observation = [0 if x<0 else x for x in discrete_observation]\n",
    "    return discrete_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_possible_states():\n",
    "    digits = len(str(STATES_IN_INTERVAL))\n",
    "    state_indices = [str(state_index).zfill(digits) for state_index in range(STATES_IN_INTERVAL)] # all encodings for a single observation variable\n",
    "    states = [state_indices for i in range(OBSERVATION_SPACE)] # for each observation variable a list of its encodings\n",
    "    states = list(itertools.product(*states)) # get all permutation of all state encodings (->list of tuples)\n",
    "    states = [''.join(x) for x in states] # join tuples to a single string\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_to_state(observation):\n",
    "    discrete_observation = discretize_observation(observation)\n",
    "    digits = len(str(STATES_IN_INTERVAL))\n",
    "    \n",
    "    state = ''\n",
    "    for state_id in discrete_observation:\n",
    "        if len(str(state_id)) < digits:\n",
    "            state += str(state_id).zfill(digits)\n",
    "        else:\n",
    "            state += str(state_id)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env._max_episode_steps = 5000\n",
    "ACTION_SPACE = env.action_space.n #number of possible actions\n",
    "OBSERVATION_SPACE = env.observation_space.shape[0] #number of observable variables\n",
    "STATES_IN_INTERVAL = 15\n",
    "#LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "EXPLORATION_PROB = 1.0 # EPSILON GREEDY STRATEGY\n",
    "EXPLORATION_DECAY = 0.995\n",
    "EXPLORATION_STOP = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_utility_table():\n",
    "    states = get_all_possible_states()\n",
    "    #utility_values = np.zeros(len(states))\n",
    "    utility_values = np.empty(len(states))\n",
    "    utility_values.fill(100)\n",
    "    utility_table = dict(zip(states, utility_values))\n",
    "    return utility_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reward_table():\n",
    "    states = get_all_possible_states()\n",
    "    rewards = np.zeros(len(states)) # init with zero; high; random\n",
    "    reward_table = dict(zip(states, rewards))\n",
    "    return reward_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_utility_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_number_state_action_table(nsa_table, state, action):\n",
    "#     key = (state, action)\n",
    "#     if key in nsa_table.keys():\n",
    "#         nsa_table[key] += 1\n",
    "#     else:\n",
    "#         nsa_table[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_number_state_action_next_state_table(nsas_table, state, action, next_state):\n",
    "    if (state, action) in nsas_table.keys():\n",
    "        if next_state in nsas_table[(state, action)].keys():\n",
    "            nsas_table[(state, action)][next_state] += 1\n",
    "        else:\n",
    "            nsas_table[(state, action)][next_state] = 1\n",
    "    else:\n",
    "        nsas_table[(state, action)] = {}\n",
    "        nsas_table[(state, action)][next_state] = 1\n",
    "        \n",
    "    return nsas_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_probability(nsas_table, state, action):\n",
    "    next_states = nsas_table[(state, action)]\n",
    "    temp = {}\n",
    "    for next_state in next_states:\n",
    "        temp[next_state] = nsas_table[(state, action)][next_state]/sum(nsas_table[(state, action)].values())\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nsa(nsas_table, state, action):\n",
    "    return sum(nsas_table[(state, action)].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_utility_estimate(utility_table, nsas_table, state, action, next_state, reward_table, epsilon, gamma):\n",
    "    next_states = nsas_table[(state, action)].keys()\n",
    "    u = 0\n",
    "    probs = get_transition_probability(nsas_table, state, action)\n",
    "    for next_state in next_states:\n",
    "        u +=  probs[next_state] * utility_table[next_state] ##\n",
    "    \n",
    "    actions = [0, 1]\n",
    "    f_values = []\n",
    "    if (state, actions[0]) in nsas_table.keys():\n",
    "        f_values.append(exploration_function(u, get_nsa(nsas_table, state, action), epsilon))\n",
    "    if (state, actions[1]) in nsas_table.keys():\n",
    "        f_values.append(exploration_function(u, get_nsa(nsas_table, state, action), epsilon))\n",
    "    if not f_values:\n",
    "        print('(O.O) we have a problem')\n",
    "    \n",
    "    utility_table[state] = reward_table[state] + gamma * max(f_values)\n",
    "    return utility_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_function(utility, n, epsilon):\n",
    "    if n < epsilon:\n",
    "        return 100\n",
    "    else:\n",
    "        return utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(utility_table, nsas_table, state): \n",
    "    best_a0, best_a1 = -100, -100\n",
    "    if (state, 0) in nsas_table.keys():\n",
    "        next_states_a0 = nsas_table[(state, 0)].keys()\n",
    "        best_a0 = max([utility_table[s] for s in next_states_a0])\n",
    "    if (state, 1) in nsas_table.keys():\n",
    "        next_states_a1 = nsas_table[(state, 1)].keys()\n",
    "        best_a1 = max([utility_table[s] for s in next_states_a1])\n",
    "    \n",
    "    if best_a0 == best_a1:\n",
    "        return randint(0,1)\n",
    "    \n",
    "    return (0 if best_a0 > best_a1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bernhard\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\bernhard\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100 Epsilon: 0.603 Mean-Reward: 42.06 Max-Reward: 149.0\n",
      "Episode: 200 Epsilon: 0.365 Mean-Reward: 55.91 Max-Reward: 152.0\n",
      "Episode: 300 Epsilon: 0.221 Mean-Reward: 54.42 Max-Reward: 157.0\n",
      "Episode: 400 Epsilon: 0.134 Mean-Reward: 79.88 Max-Reward: 165.0\n",
      "Episode: 500 Epsilon: 0.081 Mean-Reward: 86.17 Max-Reward: 182.0\n",
      "Episode: 600 Epsilon: 0.049 Mean-Reward: 81.89 Max-Reward: 192.0\n",
      "Episode: 700 Epsilon: 0.03 Mean-Reward: 95.62 Max-Reward: 175.0\n",
      "Episode: 800 Epsilon: 0.018 Mean-Reward: 83.51 Max-Reward: 145.0\n",
      "Episode: 900 Epsilon: 0.011 Mean-Reward: 99.84 Max-Reward: 173.0\n",
      "Episode: 1000 Epsilon: 0.01 Mean-Reward: 95.51 Max-Reward: 153.0\n",
      "Episode: 1100 Epsilon: 0.01 Mean-Reward: 97.28 Max-Reward: 190.0\n",
      "Episode: 1200 Epsilon: 0.01 Mean-Reward: 95.96 Max-Reward: 226.0\n",
      "Episode: 1300 Epsilon: 0.01 Mean-Reward: 90.65 Max-Reward: 223.0\n",
      "Episode: 1400 Epsilon: 0.01 Mean-Reward: 90.64 Max-Reward: 205.0\n",
      "Episode: 1500 Epsilon: 0.01 Mean-Reward: 97.46 Max-Reward: 180.0\n",
      "Episode: 1600 Epsilon: 0.01 Mean-Reward: 93.9 Max-Reward: 181.0\n",
      "Episode: 1700 Epsilon: 0.01 Mean-Reward: 99.18 Max-Reward: 191.0\n",
      "Episode: 1800 Epsilon: 0.01 Mean-Reward: 111.18 Max-Reward: 245.0\n",
      "Episode: 1900 Epsilon: 0.01 Mean-Reward: 112.29 Max-Reward: 200.0\n",
      "Episode: 2000 Epsilon: 0.01 Mean-Reward: 133.41 Max-Reward: 319.0\n",
      "Episode: 2100 Epsilon: 0.01 Mean-Reward: 121.34 Max-Reward: 276.0\n",
      "Episode: 2200 Epsilon: 0.01 Mean-Reward: 134.09 Max-Reward: 278.0\n",
      "Episode: 2300 Epsilon: 0.01 Mean-Reward: 127.73 Max-Reward: 245.0\n",
      "Episode: 2400 Epsilon: 0.01 Mean-Reward: 127.55 Max-Reward: 303.0\n",
      "Episode: 2500 Epsilon: 0.01 Mean-Reward: 138.62 Max-Reward: 296.0\n",
      "Episode: 2600 Epsilon: 0.01 Mean-Reward: 133.28 Max-Reward: 278.0\n",
      "Episode: 2700 Epsilon: 0.01 Mean-Reward: 124.2 Max-Reward: 246.0\n",
      "Episode: 2800 Epsilon: 0.01 Mean-Reward: 139.38 Max-Reward: 313.0\n",
      "Episode: 2900 Epsilon: 0.01 Mean-Reward: 140.24 Max-Reward: 295.0\n",
      "Episode: 3000 Epsilon: 0.01 Mean-Reward: 139.59 Max-Reward: 442.0\n",
      "Episode: 3100 Epsilon: 0.01 Mean-Reward: 141.95 Max-Reward: 438.0\n",
      "Episode: 3200 Epsilon: 0.01 Mean-Reward: 136.15 Max-Reward: 435.0\n",
      "Episode: 3300 Epsilon: 0.01 Mean-Reward: 147.8 Max-Reward: 330.0\n",
      "Episode: 3400 Epsilon: 0.01 Mean-Reward: 139.64 Max-Reward: 453.0\n",
      "Episode: 3500 Epsilon: 0.01 Mean-Reward: 127.83 Max-Reward: 286.0\n",
      "Episode: 3600 Epsilon: 0.01 Mean-Reward: 136.88 Max-Reward: 548.0\n",
      "Episode: 3700 Epsilon: 0.01 Mean-Reward: 144.77 Max-Reward: 475.0\n",
      "Episode: 3800 Epsilon: 0.01 Mean-Reward: 153.02 Max-Reward: 351.0\n",
      "Episode: 3900 Epsilon: 0.01 Mean-Reward: 150.18 Max-Reward: 287.0\n",
      "Episode: 4000 Epsilon: 0.01 Mean-Reward: 146.02 Max-Reward: 297.0\n",
      "Episode: 4100 Epsilon: 0.01 Mean-Reward: 138.91 Max-Reward: 295.0\n",
      "Episode: 4200 Epsilon: 0.01 Mean-Reward: 148.09 Max-Reward: 525.0\n",
      "Episode: 4300 Epsilon: 0.01 Mean-Reward: 143.4 Max-Reward: 319.0\n",
      "Episode: 4400 Epsilon: 0.01 Mean-Reward: 148.96 Max-Reward: 547.0\n",
      "Episode: 4500 Epsilon: 0.01 Mean-Reward: 150.99 Max-Reward: 324.0\n",
      "Episode: 4600 Epsilon: 0.01 Mean-Reward: 158.31 Max-Reward: 672.0\n",
      "Episode: 4700 Epsilon: 0.01 Mean-Reward: 163.6 Max-Reward: 937.0\n",
      "Episode: 4800 Epsilon: 0.01 Mean-Reward: 143.24 Max-Reward: 594.0\n",
      "Episode: 4900 Epsilon: 0.01 Mean-Reward: 146.28 Max-Reward: 609.0\n",
      "Episode: 5000 Epsilon: 0.01 Mean-Reward: 153.23 Max-Reward: 669.0\n",
      "Episode: 5100 Epsilon: 0.01 Mean-Reward: 174.75 Max-Reward: 533.0\n",
      "Episode: 5200 Epsilon: 0.01 Mean-Reward: 132.95 Max-Reward: 301.0\n",
      "Episode: 5300 Epsilon: 0.01 Mean-Reward: 146.54 Max-Reward: 479.0\n",
      "Episode: 5400 Epsilon: 0.01 Mean-Reward: 151.65 Max-Reward: 499.0\n",
      "Episode: 5500 Epsilon: 0.01 Mean-Reward: 140.78 Max-Reward: 480.0\n",
      "Episode: 5600 Epsilon: 0.01 Mean-Reward: 147.94 Max-Reward: 604.0\n",
      "Episode: 5700 Epsilon: 0.01 Mean-Reward: 144.29 Max-Reward: 473.0\n",
      "Episode: 5800 Epsilon: 0.01 Mean-Reward: 146.04 Max-Reward: 371.0\n",
      "Episode: 5900 Epsilon: 0.01 Mean-Reward: 161.32 Max-Reward: 424.0\n",
      "Episode: 6000 Epsilon: 0.01 Mean-Reward: 146.8 Max-Reward: 383.0\n",
      "Episode: 6100 Epsilon: 0.01 Mean-Reward: 143.96 Max-Reward: 547.0\n",
      "Episode: 6200 Epsilon: 0.01 Mean-Reward: 171.64 Max-Reward: 1190.0\n",
      "Episode: 6300 Epsilon: 0.01 Mean-Reward: 159.37 Max-Reward: 711.0\n",
      "Episode: 6400 Epsilon: 0.01 Mean-Reward: 147.0 Max-Reward: 372.0\n",
      "Episode: 6500 Epsilon: 0.01 Mean-Reward: 146.42 Max-Reward: 581.0\n",
      "Episode: 6600 Epsilon: 0.01 Mean-Reward: 158.03 Max-Reward: 453.0\n",
      "Episode: 6700 Epsilon: 0.01 Mean-Reward: 151.24 Max-Reward: 659.0\n",
      "Episode: 6800 Epsilon: 0.01 Mean-Reward: 146.11 Max-Reward: 404.0\n",
      "Episode: 6900 Epsilon: 0.01 Mean-Reward: 160.34 Max-Reward: 779.0\n",
      "Episode: 7000 Epsilon: 0.01 Mean-Reward: 161.94 Max-Reward: 569.0\n",
      "Episode: 7100 Epsilon: 0.01 Mean-Reward: 153.87 Max-Reward: 590.0\n",
      "Episode: 7200 Epsilon: 0.01 Mean-Reward: 146.36 Max-Reward: 445.0\n",
      "Episode: 7300 Epsilon: 0.01 Mean-Reward: 166.21 Max-Reward: 553.0\n",
      "Episode: 7400 Epsilon: 0.01 Mean-Reward: 152.98 Max-Reward: 413.0\n",
      "Episode: 7500 Epsilon: 0.01 Mean-Reward: 162.61 Max-Reward: 437.0\n",
      "Episode: 7600 Epsilon: 0.01 Mean-Reward: 144.53 Max-Reward: 422.0\n",
      "Episode: 7700 Epsilon: 0.01 Mean-Reward: 153.6 Max-Reward: 479.0\n",
      "Episode: 7800 Epsilon: 0.01 Mean-Reward: 165.66 Max-Reward: 633.0\n",
      "Episode: 7900 Epsilon: 0.01 Mean-Reward: 161.7 Max-Reward: 555.0\n",
      "Episode: 8000 Epsilon: 0.01 Mean-Reward: 163.17 Max-Reward: 523.0\n",
      "Episode: 8100 Epsilon: 0.01 Mean-Reward: 154.49 Max-Reward: 611.0\n",
      "Episode: 8200 Epsilon: 0.01 Mean-Reward: 144.07 Max-Reward: 537.0\n",
      "Episode: 8300 Epsilon: 0.01 Mean-Reward: 158.46 Max-Reward: 591.0\n",
      "Episode: 8400 Epsilon: 0.01 Mean-Reward: 171.17 Max-Reward: 680.0\n",
      "Episode: 8500 Epsilon: 0.01 Mean-Reward: 161.83 Max-Reward: 823.0\n",
      "Episode: 8600 Epsilon: 0.01 Mean-Reward: 166.49 Max-Reward: 500.0\n",
      "Episode: 8700 Epsilon: 0.01 Mean-Reward: 167.67 Max-Reward: 894.0\n",
      "Episode: 8800 Epsilon: 0.01 Mean-Reward: 149.31 Max-Reward: 485.0\n",
      "Episode: 8900 Epsilon: 0.01 Mean-Reward: 155.32 Max-Reward: 457.0\n",
      "Episode: 9000 Epsilon: 0.01 Mean-Reward: 147.9 Max-Reward: 372.0\n",
      "Episode: 9100 Epsilon: 0.01 Mean-Reward: 157.97 Max-Reward: 486.0\n",
      "Episode: 9200 Epsilon: 0.01 Mean-Reward: 164.37 Max-Reward: 582.0\n",
      "Episode: 9300 Epsilon: 0.01 Mean-Reward: 146.76 Max-Reward: 729.0\n",
      "Episode: 9400 Epsilon: 0.01 Mean-Reward: 142.48 Max-Reward: 399.0\n",
      "Episode: 9500 Epsilon: 0.01 Mean-Reward: 159.89 Max-Reward: 614.0\n"
     ]
    }
   ],
   "source": [
    "INTERVALS = create_state_intervals()\n",
    "number_of_games = 200000\n",
    "\n",
    "\n",
    "#data = pd.DataFrame(columns = ['mean', 'max', 'solve_step'])\n",
    "data_max = []\n",
    "data_mean = []\n",
    "data_solve = []\n",
    "\n",
    "solved = False\n",
    "\n",
    "for i in range(1):\n",
    "    print('EPISODE:', i)\n",
    "        # init things we dont understand\n",
    "    utility_table = create_utility_table()\n",
    "    nsas_table = {}\n",
    "    reward_table = create_reward_table()\n",
    "\n",
    "    gamma = 0.9\n",
    "    explore = 3 ## change this (jahrers idea ... critical)\n",
    "    epsilon = EXPLORATION_PROB\n",
    "    \n",
    "    if i != 0:\n",
    "        data_max.append(game_max)\n",
    "        data_mean.append(game_mean)\n",
    "        if not solved:\n",
    "            data_solve.append(-1)\n",
    "    last100_rewards = deque(maxlen=100) # fifo queue\n",
    "    game_max = []\n",
    "    game_mean = []\n",
    "    solved = False\n",
    "    for game in range(number_of_games):\n",
    "        overall_reward, done = 0, False\n",
    "        observation = env.reset()\n",
    "        state = observation_to_state(observation)\n",
    "\n",
    "        while not done:\n",
    "            if game % 1000 == 0: env.render()\n",
    "            #if np.random.rand() < epsilon:\n",
    "            #    action = env.action_space.sample()\n",
    "            #else:\n",
    "            action = get_action(utility_table, nsas_table, state)\n",
    "\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            next_state = observation_to_state(observation)\n",
    "            overall_reward += reward\n",
    "\n",
    "            if done: reward = -500 # punish if agent dies\n",
    "\n",
    "            reward_table[state] = reward\n",
    "            nsas_table = update_number_state_action_next_state_table(nsas_table, state, action, next_state)\n",
    "\n",
    "            utility_table = update_utility_estimate(utility_table, nsas_table, state, action, next_state, reward_table, explore, gamma)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        if epsilon > EXPLORATION_STOP: epsilon *= EXPLORATION_DECAY\n",
    "\n",
    "        if game % 100 == 0 and game != 0:\n",
    "            print('Episode:', game, 'Epsilon:', round(epsilon, 3), \n",
    "                  'Mean-Reward:', np.mean(last100_rewards), 'Max-Reward:', max(last100_rewards))\n",
    "        if (np.mean(last100_rewards) >= 195) and not solved: \n",
    "            print('TASK COMPLETED LAST 100 GAMES HAD AN AVERAGE SCORE >=195 ON GAME', game)\n",
    "            print(last100_rewards)\n",
    "            solved = True\n",
    "            data_solve.append(game)\n",
    "            \n",
    "        if game % 100 == 0 and game != 0:\n",
    "            game_max.append(max(last100_rewards))\n",
    "            game_mean.append(np.mean(last100_rewards))\n",
    "        \n",
    "        last100_rewards.append(overall_reward) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
