{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Greedy Utility-based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import operator\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env._max_episode_steps = 5000\n",
    "ACTION_SPACE = env.action_space.n # number of possible actions\n",
    "OBSERVATION_SPACE = env.observation_space.shape[0] # number of observable variables\n",
    "EXPLORATION_PROB = 1.0 # EPSILON GREEDY STRATEGY\n",
    "EXPLORATION_DECAY = 0.9995\n",
    "EXPLORATION_STOP = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_intervals():\n",
    "    intervals = np.zeros((OBSERVATION_SPACE, STATES_IN_INTERVAL))\n",
    "    intervals[0] = np.linspace(-4.8, 4.8, STATES_IN_INTERVAL)\n",
    "    intervals[1] = np.linspace(-3.5, 3.5, STATES_IN_INTERVAL)\n",
    "    intervals[2] = np.linspace(-0.42, 0.42, STATES_IN_INTERVAL)\n",
    "    intervals[3] = np.linspace(-4, 4, STATES_IN_INTERVAL)\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_observation(observation):\n",
    "    discrete_observation = np.array([np.digitize(observation[index], INTERVALS[index])-1 for index in range(OBSERVATION_SPACE)])\n",
    "    # if some value is under the lower border ignore it and give it min value\n",
    "    discrete_observation = [0 if x<0 else x for x in discrete_observation]\n",
    "    return discrete_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_possible_states():\n",
    "    digits = len(str(STATES_IN_INTERVAL))\n",
    "    state_indices = [str(state_index).zfill(digits) for state_index in range(STATES_IN_INTERVAL)] # all encodings for a single observation variable\n",
    "    states = [state_indices for i in range(OBSERVATION_SPACE)] # for each observation variable a list of its encodings\n",
    "    states = list(itertools.product(*states)) # get all permutation of all state encodings (->list of tuples)\n",
    "    states = [''.join(x) for x in states] # join tuples to a single string\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_to_state(observation):\n",
    "    discrete_observation = discretize_observation(observation)\n",
    "    digits = len(str(STATES_IN_INTERVAL))\n",
    "    \n",
    "    state = ''\n",
    "    for state_id in discrete_observation:\n",
    "        if len(str(state_id)) < digits:\n",
    "            state += str(state_id).zfill(digits)\n",
    "        else:\n",
    "            state += str(state_id)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env._max_episode_steps = 5000\n",
    "ACTION_SPACE = env.action_space.n #number of possible actions\n",
    "OBSERVATION_SPACE = env.observation_space.shape[0] #number of observable variables\n",
    "STATES_IN_INTERVAL = 10\n",
    "#LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "EXPLORATION_PROB = 1.0 # EPSILON GREEDY STRATEGY\n",
    "EXPLORATION_DECAY = 0.995\n",
    "EXPLORATION_STOP = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_utility_table():\n",
    "    states = get_all_possible_states()\n",
    "    #utility_values = np.zeros(len(states))\n",
    "    utility_values = np.empty(len(states))\n",
    "    utility_values.fill(100)\n",
    "    utility_table = dict(zip(states, utility_values))\n",
    "    return utility_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reward_table():\n",
    "    states = get_all_possible_states()\n",
    "    rewards = np.zeros(len(states)) # init with zero; high; random\n",
    "    reward_table = dict(zip(states, rewards))\n",
    "    return reward_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_utility_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_number_state_action_table(nsa_table, state, action):\n",
    "#     key = (state, action)\n",
    "#     if key in nsa_table.keys():\n",
    "#         nsa_table[key] += 1\n",
    "#     else:\n",
    "#         nsa_table[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_number_state_action_next_state_table(nsas_table, state, action, next_state):\n",
    "    if (state, action) in nsas_table.keys():\n",
    "        if next_state in nsas_table[(state, action)].keys():\n",
    "            nsas_table[(state, action)][next_state] += 1\n",
    "        else:\n",
    "            nsas_table[(state, action)][next_state] = 1\n",
    "    else:\n",
    "        nsas_table[(state, action)] = {}\n",
    "        nsas_table[(state, action)][next_state] = 1\n",
    "        \n",
    "    return nsas_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_probability(nsas_table, state, action):\n",
    "    next_states = nsas_table[(state, action)]\n",
    "    temp = {}\n",
    "    for next_state in next_states:\n",
    "        temp[next_state] = nsas_table[(state, action)][next_state]/sum(nsas_table[(state, action)].values())\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nsa(nsas_table, state, action):\n",
    "    return sum(nsas_table[(state, action)].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_utility_estimate(utility_table, nsas_table, state, action, next_state, reward_table, epsilon, gamma):\n",
    "    next_states = nsas_table[(state, action)].keys()\n",
    "    u = 0\n",
    "    probs = get_transition_probability(nsas_table, state, action)\n",
    "    for next_state in next_states:\n",
    "        u +=  probs[next_state] * utility_table[next_state] ##\n",
    "    \n",
    "    actions = [0, 1]\n",
    "    f_values = []\n",
    "    if (state, actions[0]) in nsas_table.keys():\n",
    "        f_values.append(exploration_function(u, get_nsa(nsas_table, state, action), epsilon))\n",
    "    if (state, actions[1]) in nsas_table.keys():\n",
    "        f_values.append(exploration_function(u, get_nsa(nsas_table, state, action), epsilon))\n",
    "    if not f_values:\n",
    "        print('(O.O) we have a problem')\n",
    "    \n",
    "    utility_table[state] = reward_table[state] + gamma * max(f_values)\n",
    "    return utility_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_function(utility, n, epsilon):\n",
    "    if n < epsilon:\n",
    "        return 100\n",
    "    else:\n",
    "        return utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(utility_table, nsas_table, state): \n",
    "    best_a0, best_a1 = -100, -100\n",
    "    if (state, 0) in nsas_table.keys():\n",
    "        next_states_a0 = nsas_table[(state, 0)].keys()\n",
    "        best_a0 = max([utility_table[s] for s in next_states_a0])\n",
    "    if (state, 1) in nsas_table.keys():\n",
    "        next_states_a1 = nsas_table[(state, 1)].keys()\n",
    "        best_a1 = max([utility_table[s] for s in next_states_a1])\n",
    "    \n",
    "    if best_a0 == best_a1:\n",
    "        return randint(0,1)\n",
    "    \n",
    "    return (0 if best_a0 > best_a1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bernhard\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\bernhard\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK COMPLETED LAST 100 GAMES HAD AN AVERAGE SCORE >=195 ON GAME 84\n",
      "deque([42.0, 12.0, 9.0, 31.0, 9.0, 9.0, 35.0, 40.0, 39.0, 10.0, 11.0, 15.0, 11.0, 32.0, 12.0, 10.0, 10.0, 25.0, 10.0, 14.0, 30.0, 10.0, 35.0, 30.0, 39.0, 32.0, 71.0, 108.0, 43.0, 97.0, 28.0, 95.0, 167.0, 221.0, 219.0, 307.0, 223.0, 307.0, 151.0, 474.0, 155.0, 1385.0, 163.0, 185.0, 187.0, 225.0, 261.0, 188.0, 202.0, 263.0, 189.0, 187.0, 224.0, 407.0, 638.0, 422.0, 313.0, 157.0, 231.0, 423.0, 195.0, 198.0, 217.0, 198.0, 237.0, 533.0, 130.0, 215.0, 235.0, 248.0, 176.0, 208.0, 214.0, 607.0, 198.0, 692.0, 173.0, 215.0, 659.0, 289.0, 290.0, 246.0, 286.0, 268.0], maxlen=100)\n",
      "Episode: 100 Epsilon: 0.603 Mean-Reward: 218.84 Max-Reward: 1385.0\n",
      "Episode: 200 Epsilon: 0.365 Mean-Reward: 334.43 Max-Reward: 925.0\n",
      "Episode: 300 Epsilon: 0.221 Mean-Reward: 397.81 Max-Reward: 2042.0\n",
      "Episode: 400 Epsilon: 0.134 Mean-Reward: 347.89 Max-Reward: 1605.0\n",
      "Episode: 500 Epsilon: 0.081 Mean-Reward: 361.15 Max-Reward: 1552.0\n",
      "Episode: 600 Epsilon: 0.049 Mean-Reward: 384.71 Max-Reward: 1354.0\n",
      "Episode: 700 Epsilon: 0.03 Mean-Reward: 345.16 Max-Reward: 1044.0\n",
      "Episode: 800 Epsilon: 0.018 Mean-Reward: 387.68 Max-Reward: 1446.0\n",
      "Episode: 900 Epsilon: 0.011 Mean-Reward: 356.75 Max-Reward: 1029.0\n",
      "Episode: 1000 Epsilon: 0.01 Mean-Reward: 437.0 Max-Reward: 2677.0\n",
      "Episode: 1100 Epsilon: 0.01 Mean-Reward: 423.77 Max-Reward: 1832.0\n",
      "Episode: 1200 Epsilon: 0.01 Mean-Reward: 392.46 Max-Reward: 1368.0\n"
     ]
    }
   ],
   "source": [
    "INTERVALS = create_state_intervals()\n",
    "number_of_games = 200000\n",
    "\n",
    "\n",
    "#data = pd.DataFrame(columns = ['mean', 'max', 'solve_step'])\n",
    "data_max = []\n",
    "data_mean = []\n",
    "data_solve = []\n",
    "\n",
    "solved = False\n",
    "\n",
    "for i in range(1):\n",
    "    print('EPISODE:', i)\n",
    "        # init things we dont understand\n",
    "    utility_table = create_utility_table()\n",
    "    nsas_table = {}\n",
    "    reward_table = create_reward_table()\n",
    "\n",
    "    gamma = 0.9\n",
    "    explore = 1 ## change this (jahrers idea ... critical)\n",
    "    epsilon = EXPLORATION_PROB\n",
    "    \n",
    "    if i != 0:\n",
    "        data_max.append(game_max)\n",
    "        data_mean.append(game_mean)\n",
    "        if not solved:\n",
    "            data_solve.append(-1)\n",
    "    last100_rewards = deque(maxlen=100) # fifo queue\n",
    "    game_max = []\n",
    "    game_mean = []\n",
    "    solved = False\n",
    "    for game in range(number_of_games):\n",
    "        overall_reward, done = 0, False\n",
    "        observation = env.reset()\n",
    "        state = observation_to_state(observation)\n",
    "\n",
    "        while not done:\n",
    "            #if game % 1000 == 0: env.render()\n",
    "            #if np.random.rand() < epsilon:\n",
    "            #    action = env.action_space.sample()\n",
    "            #else:\n",
    "            action = get_action(utility_table, nsas_table, state)\n",
    "\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            next_state = observation_to_state(observation)\n",
    "            overall_reward += reward\n",
    "\n",
    "            if done: reward = -500 # punish if agent dies\n",
    "\n",
    "            reward_table[state] = reward\n",
    "            nsas_table = update_number_state_action_next_state_table(nsas_table, state, action, next_state)\n",
    "\n",
    "            utility_table = update_utility_estimate(utility_table, nsas_table, state, action, next_state, reward_table, explore, gamma)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        if epsilon > EXPLORATION_STOP: epsilon *= EXPLORATION_DECAY\n",
    "\n",
    "        if game % 100 == 0 and game != 0:\n",
    "            print('Episode:', game, 'Epsilon:', round(epsilon, 3), \n",
    "                  'Mean-Reward:', np.mean(last100_rewards), 'Max-Reward:', max(last100_rewards))\n",
    "        if (np.mean(last100_rewards) >= 195) and not solved: \n",
    "            print('TASK COMPLETED LAST 100 GAMES HAD AN AVERAGE SCORE >=195 ON GAME', game)\n",
    "            print(last100_rewards)\n",
    "            solved = True\n",
    "            data_solve.append(game)\n",
    "            \n",
    "        if game % 100 == 0 and game != 0:\n",
    "            game_max.append(max(last100_rewards))\n",
    "            game_mean.append(np.mean(last100_rewards))\n",
    "        \n",
    "        last100_rewards.append(overall_reward) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
