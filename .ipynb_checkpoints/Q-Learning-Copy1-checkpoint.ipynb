{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Dependent Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import gym \n",
    "import operator\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env._max_episode_steps = 5000\n",
    "number_of_games = 10000000\n",
    "ACTION_SPACE = env.action_space.n #number of possible actions\n",
    "OBSERVATION_SPACE = env.observation_space.shape[0] #number of observable variables\n",
    "STATES_IN_INTERVAL = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def create_state_intervals():\n",
    "    intervals = np.zeros((OBSERVATION_SPACE, STATES_IN_INTERVAL))\n",
    "    intervals[0] = np.linspace(-4.8, 4.8, STATES_IN_INTERVAL)\n",
    "    intervals[1] = np.linspace(-3.5, 3.5, STATES_IN_INTERVAL)\n",
    "    intervals[2] = np.linspace(-0.42, 0.42, STATES_IN_INTERVAL)\n",
    "    intervals[3] = np.linspace(-4, 4, STATES_IN_INTERVAL)\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def discretize_observation(observation):\n",
    "    discrete_observation = np.array([np.digitize(observation[index], INTERVALS[index])-1 for index in range(OBSERVATION_SPACE)])\n",
    "    # if some value is under the lower border ignore it and give it min value\n",
    "    discrete_observation = [0 if x<0 else x for x in discrete_observation]\n",
    "    return discrete_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_all_possible_states():\n",
    "    digits = len(str(STATES_IN_INTERVAL))\n",
    "    state_indices = [str(state_index).zfill(digits) for state_index in range(STATES_IN_INTERVAL)] # all encodings for a single observation variable\n",
    "    states = [state_indices for i in range(OBSERVATION_SPACE)] # for each observation variable a list of its encodings\n",
    "    states = list(itertools.product(*states)) # get all permutation of all state encodings (->list of tuples)\n",
    "    states = [''.join(x) for x in states] # join tuples to a single string\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def observation_to_state(observation):\n",
    "    discrete_observation = discretize_observation(observation)\n",
    "    digits = len(str(STATES_IN_INTERVAL))\n",
    "    \n",
    "    state = ''\n",
    "    for state_id in discrete_observation:\n",
    "        if len(str(state_id)) < digits:\n",
    "            state += str(state_id).zfill(digits)\n",
    "        else:\n",
    "            state += str(state_id)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_q_table(states, actions):\n",
    "    q_table = dict()\n",
    "    for state in states:\n",
    "        q_table[state] = dict()\n",
    "        for action in actions:\n",
    "            q_table[state][action] = np.random.randint(10)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(q_table, state, alpha):\n",
    "    action = 0 if q_table[state][0] > q_table[state][1] else 1\n",
    "    if(random.random() < alpha):\n",
    "        action += 1 \n",
    "        action %= 2\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_a(q_table, next_state):\n",
    "    return max(q_table[next_state][k] for k in q_table[next_state].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(q_table, state, action, next_state, reward, alpha, gamma):\n",
    "    q_s_a = q_table[state][action]\n",
    "    q_table[state][action] = q_s_a + alpha * (reward + gamma * max_a(q_table, next_state) - q_s_a)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bejahrer\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\bejahrer\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100 Mean-Reward: 17.47 Max-Reward: 64.0 Alpha: 0.9038873549665959\n",
      "Episode: 200 Mean-Reward: 14.23 Max-Reward: 50.0 Alpha: 0.8178301806491574\n",
      "Episode: 300 Mean-Reward: 14.01 Max-Reward: 42.0 Alpha: 0.7399663251239436\n",
      "Episode: 400 Mean-Reward: 14.66 Max-Reward: 44.0 Alpha: 0.6695157201007336\n",
      "Episode: 500 Mean-Reward: 16.53 Max-Reward: 34.0 Alpha: 0.6057725659163237\n",
      "Episode: 600 Mean-Reward: 18.28 Max-Reward: 50.0 Alpha: 0.548098260578011\n",
      "Episode: 700 Mean-Reward: 20.05 Max-Reward: 56.0 Alpha: 0.4959150020176678\n",
      "Episode: 800 Mean-Reward: 24.72 Max-Reward: 95.0 Alpha: 0.44869999946146477\n",
      "Episode: 900 Mean-Reward: 26.06 Max-Reward: 73.0 Alpha: 0.4059802359226587\n",
      "Episode: 1000 Mean-Reward: 33.04 Max-Reward: 193.0 Alpha: 0.36732772934619257\n",
      "Episode: 1100 Mean-Reward: 39.8 Max-Reward: 152.0 Alpha: 0.33235524492954527\n",
      "Episode: 1200 Mean-Reward: 47.81 Max-Reward: 175.0 Alpha: 0.3007124156643058\n",
      "Episode: 1300 Mean-Reward: 51.52 Max-Reward: 171.0 Alpha: 0.2720822322326576\n",
      "Episode: 1400 Mean-Reward: 70.61 Max-Reward: 230.0 Alpha: 0.2461778670932771\n",
      "Episode: 1500 Mean-Reward: 100.63 Max-Reward: 373.0 Alpha: 0.22273980093919937\n",
      "Episode: 1600 Mean-Reward: 94.91 Max-Reward: 277.0 Alpha: 0.2015332227394583\n",
      "Episode: 1700 Mean-Reward: 127.34 Max-Reward: 428.0 Alpha: 0.18234567731717977\n",
      "Episode: 1800 Mean-Reward: 157.09 Max-Reward: 351.0 Alpha: 0.1649849368967147\n",
      "Episode: 1900 Mean-Reward: 181.3 Max-Reward: 551.0 Alpha: 0.14927707529619813\n",
      "TASK COMPLETED LAST 100 GAMES HAD AN AVERAGE SCORE >=195 ON GAME 1939\n",
      "deque([156.0, 42.0, 248.0, 270.0, 49.0, 157.0, 255.0, 419.0, 135.0, 156.0, 198.0, 150.0, 272.0, 144.0, 161.0, 207.0, 551.0, 193.0, 176.0, 262.0, 159.0, 252.0, 130.0, 18.0, 178.0, 95.0, 102.0, 186.0, 125.0, 130.0, 151.0, 207.0, 368.0, 251.0, 107.0, 198.0, 281.0, 105.0, 272.0, 95.0, 232.0, 308.0, 347.0, 212.0, 151.0, 252.0, 404.0, 14.0, 310.0, 165.0, 102.0, 266.0, 92.0, 89.0, 270.0, 142.0, 132.0, 246.0, 150.0, 149.0, 190.0, 157.0, 133.0, 162.0, 184.0, 184.0, 297.0, 136.0, 232.0, 176.0, 97.0, 251.0, 308.0, 340.0, 137.0, 240.0, 181.0, 205.0, 222.0, 252.0, 127.0, 106.0, 26.0, 125.0, 202.0, 255.0, 239.0, 261.0, 59.0, 146.0, 167.0, 152.0, 288.0, 303.0, 232.0, 135.0, 265.0, 276.0, 279.0, 263.0], maxlen=100)\n",
      "Episode: 2000 Mean-Reward: 197.19 Max-Reward: 454.0 Alpha: 0.13506472547210188\n",
      "Episode: 2100 Mean-Reward: 197.91 Max-Reward: 441.0 Alpha: 0.12220550295922675\n",
      "Episode: 2200 Mean-Reward: 174.32 Max-Reward: 392.0 Alpha: 0.11057057941158951\n",
      "Episode: 2300 Mean-Reward: 189.03 Max-Reward: 595.0 Alpha: 0.10004339195341891\n",
      "Episode: 2400 Mean-Reward: 181.32 Max-Reward: 394.0 Alpha: 0.09051847541007228\n",
      "Episode: 2500 Mean-Reward: 217.4 Max-Reward: 624.0 Alpha: 0.08190040571973876\n",
      "Episode: 2600 Mean-Reward: 195.99 Max-Reward: 397.0 Alpha: 0.07410284394064628\n",
      "Episode: 2700 Mean-Reward: 209.89 Max-Reward: 499.0 Alpha: 0.06704767127628951\n",
      "Episode: 2800 Mean-Reward: 223.08 Max-Reward: 423.0 Alpha: 0.060664206453048174\n",
      "Episode: 2900 Mean-Reward: 229.77 Max-Reward: 491.0 Alpha: 0.05488849760960279\n",
      "Episode: 3000 Mean-Reward: 236.86 Max-Reward: 567.0 Alpha: 0.049662681604038215\n",
      "Episode: 3100 Mean-Reward: 275.26 Max-Reward: 630.0 Alpha: 0.04493440431994225\n",
      "Episode: 3200 Mean-Reward: 217.53 Max-Reward: 621.0 Alpha: 0.04065629616391608\n",
      "Episode: 3300 Mean-Reward: 230.48 Max-Reward: 472.0 Alpha: 0.03678549749984046\n",
      "Episode: 3400 Mean-Reward: 199.93 Max-Reward: 462.0 Alpha: 0.03328322926552661\n",
      "Episode: 3500 Mean-Reward: 240.1 Max-Reward: 583.0 Alpha: 0.030114404470033673\n",
      "Episode: 3600 Mean-Reward: 361.05 Max-Reward: 948.0 Alpha: 0.027247276679492435\n",
      "Episode: 3700 Mean-Reward: 298.83 Max-Reward: 663.0 Alpha: 0.024653121969839265\n",
      "Episode: 3800 Mean-Reward: 343.39 Max-Reward: 659.0 Alpha: 0.022305951160147018\n",
      "Episode: 3900 Mean-Reward: 307.91 Max-Reward: 729.0 Alpha: 0.02018224944360293\n",
      "Episode: 4000 Mean-Reward: 338.78 Max-Reward: 976.0 Alpha: 0.018260740807661956\n",
      "Episode: 4100 Mean-Reward: 283.89 Max-Reward: 635.0 Alpha: 0.016522174883251375\n",
      "Episode: 4200 Mean-Reward: 281.9 Max-Reward: 575.0 Alpha: 0.014949134087605212\n",
      "Episode: 4300 Mean-Reward: 259.48 Max-Reward: 482.0 Alpha: 0.01352585912861506\n",
      "Episode: 4400 Mean-Reward: 266.85 Max-Reward: 488.0 Alpha: 0.012238091122537187\n",
      "Episode: 4500 Mean-Reward: 259.9 Max-Reward: 562.0 Alpha: 0.011072928743333644\n",
      "Episode: 4600 Mean-Reward: 281.14 Max-Reward: 558.0 Alpha: 0.010018698972517958\n",
      "Episode: 4700 Mean-Reward: 249.52 Max-Reward: 470.0 Alpha: 0.009998671593271896\n",
      "Episode: 4800 Mean-Reward: 266.91 Max-Reward: 587.0 Alpha: 0.009998671593271896\n",
      "Episode: 4900 Mean-Reward: 297.94 Max-Reward: 614.0 Alpha: 0.009998671593271896\n",
      "Episode: 5000 Mean-Reward: 316.38 Max-Reward: 500.0 Alpha: 0.009998671593271896\n",
      "Episode: 5100 Mean-Reward: 310.43 Max-Reward: 654.0 Alpha: 0.009998671593271896\n",
      "Episode: 5200 Mean-Reward: 299.77 Max-Reward: 623.0 Alpha: 0.009998671593271896\n",
      "Episode: 5300 Mean-Reward: 306.92 Max-Reward: 519.0 Alpha: 0.009998671593271896\n",
      "Episode: 5400 Mean-Reward: 315.98 Max-Reward: 566.0 Alpha: 0.009998671593271896\n",
      "Episode: 5500 Mean-Reward: 327.37 Max-Reward: 1132.0 Alpha: 0.009998671593271896\n",
      "Episode: 5600 Mean-Reward: 315.34 Max-Reward: 521.0 Alpha: 0.009998671593271896\n",
      "Episode: 5700 Mean-Reward: 302.57 Max-Reward: 499.0 Alpha: 0.009998671593271896\n",
      "Episode: 5800 Mean-Reward: 312.62 Max-Reward: 486.0 Alpha: 0.009998671593271896\n",
      "Episode: 5900 Mean-Reward: 318.79 Max-Reward: 604.0 Alpha: 0.009998671593271896\n",
      "Episode: 6000 Mean-Reward: 302.64 Max-Reward: 515.0 Alpha: 0.009998671593271896\n",
      "Episode: 6100 Mean-Reward: 296.2 Max-Reward: 544.0 Alpha: 0.009998671593271896\n",
      "Episode: 6200 Mean-Reward: 325.12 Max-Reward: 529.0 Alpha: 0.009998671593271896\n",
      "Episode: 6300 Mean-Reward: 292.23 Max-Reward: 551.0 Alpha: 0.009998671593271896\n",
      "Episode: 6400 Mean-Reward: 322.18 Max-Reward: 566.0 Alpha: 0.009998671593271896\n",
      "Episode: 6500 Mean-Reward: 302.92 Max-Reward: 523.0 Alpha: 0.009998671593271896\n",
      "Episode: 6600 Mean-Reward: 331.13 Max-Reward: 730.0 Alpha: 0.009998671593271896\n",
      "Episode: 6700 Mean-Reward: 342.7 Max-Reward: 726.0 Alpha: 0.009998671593271896\n",
      "Episode: 6800 Mean-Reward: 360.73 Max-Reward: 714.0 Alpha: 0.009998671593271896\n",
      "Episode: 6900 Mean-Reward: 353.69 Max-Reward: 641.0 Alpha: 0.009998671593271896\n",
      "Episode: 7000 Mean-Reward: 349.95 Max-Reward: 864.0 Alpha: 0.009998671593271896\n",
      "Episode: 7100 Mean-Reward: 373.3 Max-Reward: 796.0 Alpha: 0.009998671593271896\n",
      "Episode: 7200 Mean-Reward: 334.89 Max-Reward: 687.0 Alpha: 0.009998671593271896\n",
      "Episode: 7300 Mean-Reward: 370.28 Max-Reward: 833.0 Alpha: 0.009998671593271896\n",
      "Episode: 7400 Mean-Reward: 351.53 Max-Reward: 673.0 Alpha: 0.009998671593271896\n",
      "Episode: 7500 Mean-Reward: 427.38 Max-Reward: 802.0 Alpha: 0.009998671593271896\n",
      "Episode: 7600 Mean-Reward: 397.14 Max-Reward: 1261.0 Alpha: 0.009998671593271896\n",
      "Episode: 7700 Mean-Reward: 287.08 Max-Reward: 564.0 Alpha: 0.009998671593271896\n",
      "Episode: 7800 Mean-Reward: 216.9 Max-Reward: 720.0 Alpha: 0.009998671593271896\n",
      "Episode: 7900 Mean-Reward: 326.94 Max-Reward: 1114.0 Alpha: 0.009998671593271896\n",
      "Episode: 8000 Mean-Reward: 400.93 Max-Reward: 911.0 Alpha: 0.009998671593271896\n",
      "Episode: 8100 Mean-Reward: 410.09 Max-Reward: 1033.0 Alpha: 0.009998671593271896\n",
      "Episode: 8200 Mean-Reward: 393.73 Max-Reward: 829.0 Alpha: 0.009998671593271896\n",
      "Episode: 8300 Mean-Reward: 370.55 Max-Reward: 705.0 Alpha: 0.009998671593271896\n",
      "Episode: 8400 Mean-Reward: 344.59 Max-Reward: 603.0 Alpha: 0.009998671593271896\n",
      "Episode: 8500 Mean-Reward: 356.92 Max-Reward: 880.0 Alpha: 0.009998671593271896\n",
      "Episode: 8600 Mean-Reward: 377.38 Max-Reward: 635.0 Alpha: 0.009998671593271896\n",
      "Episode: 8700 Mean-Reward: 288.22 Max-Reward: 724.0 Alpha: 0.009998671593271896\n",
      "Episode: 8800 Mean-Reward: 266.14 Max-Reward: 465.0 Alpha: 0.009998671593271896\n",
      "Episode: 8900 Mean-Reward: 337.63 Max-Reward: 624.0 Alpha: 0.009998671593271896\n",
      "Episode: 9000 Mean-Reward: 398.33 Max-Reward: 815.0 Alpha: 0.009998671593271896\n",
      "Episode: 9100 Mean-Reward: 380.05 Max-Reward: 698.0 Alpha: 0.009998671593271896\n",
      "Episode: 9200 Mean-Reward: 274.24 Max-Reward: 689.0 Alpha: 0.009998671593271896\n",
      "Episode: 9300 Mean-Reward: 242.8 Max-Reward: 461.0 Alpha: 0.009998671593271896\n",
      "Episode: 9400 Mean-Reward: 229.48 Max-Reward: 511.0 Alpha: 0.009998671593271896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 9500 Mean-Reward: 361.44 Max-Reward: 849.0 Alpha: 0.009998671593271896\n",
      "Episode: 9600 Mean-Reward: 342.21 Max-Reward: 747.0 Alpha: 0.009998671593271896\n",
      "Episode: 9700 Mean-Reward: 344.87 Max-Reward: 674.0 Alpha: 0.009998671593271896\n",
      "Episode: 9800 Mean-Reward: 382.44 Max-Reward: 654.0 Alpha: 0.009998671593271896\n",
      "Episode: 9900 Mean-Reward: 367.27 Max-Reward: 663.0 Alpha: 0.009998671593271896\n",
      "Episode: 10000 Mean-Reward: 364.12 Max-Reward: 737.0 Alpha: 0.009998671593271896\n",
      "Episode: 10100 Mean-Reward: 362.8 Max-Reward: 601.0 Alpha: 0.009998671593271896\n",
      "Episode: 10200 Mean-Reward: 392.11 Max-Reward: 772.0 Alpha: 0.009998671593271896\n",
      "Episode: 10300 Mean-Reward: 376.66 Max-Reward: 885.0 Alpha: 0.009998671593271896\n",
      "Episode: 10400 Mean-Reward: 337.08 Max-Reward: 748.0 Alpha: 0.009998671593271896\n",
      "Episode: 10500 Mean-Reward: 352.09 Max-Reward: 679.0 Alpha: 0.009998671593271896\n",
      "Episode: 10600 Mean-Reward: 389.69 Max-Reward: 1089.0 Alpha: 0.009998671593271896\n",
      "Episode: 10700 Mean-Reward: 346.26 Max-Reward: 656.0 Alpha: 0.009998671593271896\n",
      "Episode: 10800 Mean-Reward: 366.97 Max-Reward: 688.0 Alpha: 0.009998671593271896\n",
      "Episode: 10900 Mean-Reward: 277.17 Max-Reward: 603.0 Alpha: 0.009998671593271896\n",
      "Episode: 11000 Mean-Reward: 350.77 Max-Reward: 716.0 Alpha: 0.009998671593271896\n",
      "Episode: 11100 Mean-Reward: 326.56 Max-Reward: 656.0 Alpha: 0.009998671593271896\n",
      "Episode: 11200 Mean-Reward: 246.86 Max-Reward: 614.0 Alpha: 0.009998671593271896\n",
      "Episode: 11300 Mean-Reward: 334.92 Max-Reward: 686.0 Alpha: 0.009998671593271896\n",
      "Episode: 11400 Mean-Reward: 313.99 Max-Reward: 555.0 Alpha: 0.009998671593271896\n",
      "Episode: 11500 Mean-Reward: 358.79 Max-Reward: 796.0 Alpha: 0.009998671593271896\n",
      "Episode: 11600 Mean-Reward: 349.67 Max-Reward: 579.0 Alpha: 0.009998671593271896\n",
      "Episode: 11700 Mean-Reward: 269.51 Max-Reward: 524.0 Alpha: 0.009998671593271896\n",
      "Episode: 11800 Mean-Reward: 195.75 Max-Reward: 287.0 Alpha: 0.009998671593271896\n",
      "Episode: 11900 Mean-Reward: 190.7 Max-Reward: 423.0 Alpha: 0.009998671593271896\n",
      "Episode: 12000 Mean-Reward: 217.56 Max-Reward: 611.0 Alpha: 0.009998671593271896\n",
      "Episode: 12100 Mean-Reward: 346.47 Max-Reward: 741.0 Alpha: 0.009998671593271896\n",
      "Episode: 12200 Mean-Reward: 320.52 Max-Reward: 663.0 Alpha: 0.009998671593271896\n",
      "Episode: 12300 Mean-Reward: 446.69 Max-Reward: 812.0 Alpha: 0.009998671593271896\n",
      "Episode: 12400 Mean-Reward: 422.39 Max-Reward: 667.0 Alpha: 0.009998671593271896\n",
      "Episode: 12500 Mean-Reward: 425.17 Max-Reward: 608.0 Alpha: 0.009998671593271896\n",
      "Episode: 12600 Mean-Reward: 425.76 Max-Reward: 729.0 Alpha: 0.009998671593271896\n",
      "Episode: 12700 Mean-Reward: 431.57 Max-Reward: 745.0 Alpha: 0.009998671593271896\n",
      "Episode: 12800 Mean-Reward: 439.44 Max-Reward: 622.0 Alpha: 0.009998671593271896\n",
      "Episode: 12900 Mean-Reward: 460.48 Max-Reward: 754.0 Alpha: 0.009998671593271896\n",
      "Episode: 13000 Mean-Reward: 464.28 Max-Reward: 761.0 Alpha: 0.009998671593271896\n",
      "Episode: 13100 Mean-Reward: 458.46 Max-Reward: 867.0 Alpha: 0.009998671593271896\n",
      "Episode: 13200 Mean-Reward: 461.54 Max-Reward: 867.0 Alpha: 0.009998671593271896\n",
      "Episode: 13300 Mean-Reward: 506.16 Max-Reward: 827.0 Alpha: 0.009998671593271896\n",
      "Episode: 13400 Mean-Reward: 480.6 Max-Reward: 845.0 Alpha: 0.009998671593271896\n",
      "Episode: 13500 Mean-Reward: 480.85 Max-Reward: 757.0 Alpha: 0.009998671593271896\n",
      "Episode: 13600 Mean-Reward: 452.47 Max-Reward: 632.0 Alpha: 0.009998671593271896\n",
      "Episode: 13700 Mean-Reward: 469.26 Max-Reward: 840.0 Alpha: 0.009998671593271896\n",
      "Episode: 13800 Mean-Reward: 486.0 Max-Reward: 912.0 Alpha: 0.009998671593271896\n",
      "Episode: 13900 Mean-Reward: 480.96 Max-Reward: 972.0 Alpha: 0.009998671593271896\n",
      "Episode: 14000 Mean-Reward: 471.88 Max-Reward: 914.0 Alpha: 0.009998671593271896\n",
      "Episode: 14100 Mean-Reward: 461.96 Max-Reward: 754.0 Alpha: 0.009998671593271896\n",
      "Episode: 14200 Mean-Reward: 458.0 Max-Reward: 824.0 Alpha: 0.009998671593271896\n",
      "Episode: 14300 Mean-Reward: 493.9 Max-Reward: 985.0 Alpha: 0.009998671593271896\n",
      "Episode: 14400 Mean-Reward: 490.7 Max-Reward: 1120.0 Alpha: 0.009998671593271896\n",
      "Episode: 14500 Mean-Reward: 506.39 Max-Reward: 934.0 Alpha: 0.009998671593271896\n",
      "Episode: 14600 Mean-Reward: 468.31 Max-Reward: 1059.0 Alpha: 0.009998671593271896\n",
      "Episode: 14700 Mean-Reward: 447.08 Max-Reward: 761.0 Alpha: 0.009998671593271896\n",
      "Episode: 14800 Mean-Reward: 392.92 Max-Reward: 779.0 Alpha: 0.009998671593271896\n",
      "Episode: 14900 Mean-Reward: 366.71 Max-Reward: 999.0 Alpha: 0.009998671593271896\n",
      "Episode: 15000 Mean-Reward: 427.51 Max-Reward: 921.0 Alpha: 0.009998671593271896\n",
      "Episode: 15100 Mean-Reward: 379.41 Max-Reward: 802.0 Alpha: 0.009998671593271896\n",
      "Episode: 15200 Mean-Reward: 450.33 Max-Reward: 746.0 Alpha: 0.009998671593271896\n",
      "Episode: 15300 Mean-Reward: 377.15 Max-Reward: 710.0 Alpha: 0.009998671593271896\n",
      "Episode: 15400 Mean-Reward: 338.48 Max-Reward: 673.0 Alpha: 0.009998671593271896\n",
      "Episode: 15500 Mean-Reward: 357.3 Max-Reward: 579.0 Alpha: 0.009998671593271896\n",
      "Episode: 15600 Mean-Reward: 349.15 Max-Reward: 485.0 Alpha: 0.009998671593271896\n",
      "Episode: 15700 Mean-Reward: 352.48 Max-Reward: 589.0 Alpha: 0.009998671593271896\n",
      "Episode: 15800 Mean-Reward: 342.45 Max-Reward: 537.0 Alpha: 0.009998671593271896\n",
      "Episode: 15900 Mean-Reward: 344.95 Max-Reward: 451.0 Alpha: 0.009998671593271896\n",
      "Episode: 16000 Mean-Reward: 377.88 Max-Reward: 630.0 Alpha: 0.009998671593271896\n",
      "Episode: 16100 Mean-Reward: 353.49 Max-Reward: 552.0 Alpha: 0.009998671593271896\n",
      "Episode: 16200 Mean-Reward: 357.34 Max-Reward: 514.0 Alpha: 0.009998671593271896\n",
      "Episode: 16300 Mean-Reward: 399.91 Max-Reward: 860.0 Alpha: 0.009998671593271896\n",
      "Episode: 16400 Mean-Reward: 386.06 Max-Reward: 621.0 Alpha: 0.009998671593271896\n",
      "Episode: 16500 Mean-Reward: 382.9 Max-Reward: 583.0 Alpha: 0.009998671593271896\n",
      "Episode: 16600 Mean-Reward: 374.29 Max-Reward: 863.0 Alpha: 0.009998671593271896\n",
      "Episode: 16700 Mean-Reward: 394.18 Max-Reward: 666.0 Alpha: 0.009998671593271896\n",
      "Episode: 16800 Mean-Reward: 518.66 Max-Reward: 885.0 Alpha: 0.009998671593271896\n",
      "Episode: 16900 Mean-Reward: 525.7 Max-Reward: 1223.0 Alpha: 0.009998671593271896\n",
      "Episode: 17000 Mean-Reward: 493.86 Max-Reward: 977.0 Alpha: 0.009998671593271896\n",
      "Episode: 17100 Mean-Reward: 482.26 Max-Reward: 883.0 Alpha: 0.009998671593271896\n",
      "Episode: 17200 Mean-Reward: 516.53 Max-Reward: 869.0 Alpha: 0.009998671593271896\n",
      "Episode: 17300 Mean-Reward: 549.97 Max-Reward: 951.0 Alpha: 0.009998671593271896\n",
      "Episode: 17400 Mean-Reward: 552.78 Max-Reward: 916.0 Alpha: 0.009998671593271896\n",
      "Episode: 17500 Mean-Reward: 563.16 Max-Reward: 942.0 Alpha: 0.009998671593271896\n",
      "Episode: 17600 Mean-Reward: 580.67 Max-Reward: 985.0 Alpha: 0.009998671593271896\n",
      "Episode: 17700 Mean-Reward: 574.92 Max-Reward: 943.0 Alpha: 0.009998671593271896\n",
      "Episode: 17800 Mean-Reward: 560.74 Max-Reward: 920.0 Alpha: 0.009998671593271896\n",
      "Episode: 17900 Mean-Reward: 514.84 Max-Reward: 858.0 Alpha: 0.009998671593271896\n",
      "Episode: 18000 Mean-Reward: 493.53 Max-Reward: 796.0 Alpha: 0.009998671593271896\n",
      "Episode: 18100 Mean-Reward: 552.84 Max-Reward: 907.0 Alpha: 0.009998671593271896\n",
      "Episode: 18200 Mean-Reward: 551.53 Max-Reward: 819.0 Alpha: 0.009998671593271896\n",
      "Episode: 18300 Mean-Reward: 511.1 Max-Reward: 831.0 Alpha: 0.009998671593271896\n",
      "Episode: 18400 Mean-Reward: 561.75 Max-Reward: 923.0 Alpha: 0.009998671593271896\n",
      "Episode: 18500 Mean-Reward: 572.94 Max-Reward: 838.0 Alpha: 0.009998671593271896\n",
      "Episode: 18600 Mean-Reward: 573.48 Max-Reward: 842.0 Alpha: 0.009998671593271896\n",
      "Episode: 18700 Mean-Reward: 520.2 Max-Reward: 882.0 Alpha: 0.009998671593271896\n",
      "Episode: 18800 Mean-Reward: 503.23 Max-Reward: 875.0 Alpha: 0.009998671593271896\n",
      "Episode: 18900 Mean-Reward: 535.45 Max-Reward: 977.0 Alpha: 0.009998671593271896\n",
      "Episode: 19000 Mean-Reward: 510.08 Max-Reward: 863.0 Alpha: 0.009998671593271896\n",
      "Episode: 19100 Mean-Reward: 514.48 Max-Reward: 765.0 Alpha: 0.009998671593271896\n",
      "Episode: 19200 Mean-Reward: 510.24 Max-Reward: 914.0 Alpha: 0.009998671593271896\n",
      "Episode: 19300 Mean-Reward: 498.88 Max-Reward: 803.0 Alpha: 0.009998671593271896\n",
      "Episode: 19400 Mean-Reward: 532.44 Max-Reward: 832.0 Alpha: 0.009998671593271896\n",
      "Episode: 19500 Mean-Reward: 366.47 Max-Reward: 620.0 Alpha: 0.009998671593271896\n",
      "Episode: 19600 Mean-Reward: 373.23 Max-Reward: 652.0 Alpha: 0.009998671593271896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 19700 Mean-Reward: 454.68 Max-Reward: 889.0 Alpha: 0.009998671593271896\n",
      "Episode: 19800 Mean-Reward: 490.57 Max-Reward: 867.0 Alpha: 0.009998671593271896\n",
      "Episode: 19900 Mean-Reward: 501.04 Max-Reward: 918.0 Alpha: 0.009998671593271896\n",
      "Episode: 20000 Mean-Reward: 539.16 Max-Reward: 1104.0 Alpha: 0.009998671593271896\n",
      "Episode: 20100 Mean-Reward: 499.82 Max-Reward: 1290.0 Alpha: 0.009998671593271896\n",
      "Episode: 20200 Mean-Reward: 445.84 Max-Reward: 746.0 Alpha: 0.009998671593271896\n",
      "Episode: 20300 Mean-Reward: 509.95 Max-Reward: 935.0 Alpha: 0.009998671593271896\n",
      "Episode: 20400 Mean-Reward: 495.92 Max-Reward: 985.0 Alpha: 0.009998671593271896\n",
      "Episode: 20500 Mean-Reward: 499.67 Max-Reward: 832.0 Alpha: 0.009998671593271896\n",
      "Episode: 20600 Mean-Reward: 510.15 Max-Reward: 875.0 Alpha: 0.009998671593271896\n",
      "Episode: 20700 Mean-Reward: 512.72 Max-Reward: 856.0 Alpha: 0.009998671593271896\n",
      "Episode: 20800 Mean-Reward: 480.54 Max-Reward: 903.0 Alpha: 0.009998671593271896\n",
      "Episode: 20900 Mean-Reward: 495.67 Max-Reward: 1123.0 Alpha: 0.009998671593271896\n"
     ]
    }
   ],
   "source": [
    "INTERVALS = create_state_intervals()\n",
    "\n",
    "for i in range(1):\n",
    "    print('EPISODE:', i)\n",
    "            \n",
    "    last100_rewards = deque(maxlen=100) # fifo queue\n",
    "    game_max = []\n",
    "    game_mean = []\n",
    "    solved = False\n",
    "    \n",
    "    q_table = init_q_table(get_all_possible_states(), [0, 1])\n",
    "    \n",
    "    \n",
    "    alpha = 1\n",
    "    gamma = 0.9\n",
    "          \n",
    "    for game in range(number_of_games):\n",
    "        \n",
    "        overall_reward, done = 0, False\n",
    "        observation = env.reset()\n",
    "        state = observation_to_state(observation)\n",
    "        \n",
    "        if alpha > 0.01:\n",
    "            alpha *= 0.999\n",
    "            \n",
    "        while not done:\n",
    "            if game % 1000 == 0: env.render()\n",
    "                       \n",
    "            action = get_action(q_table, state, alpha)\n",
    "\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            \n",
    "            next_state = observation_to_state(observation)\n",
    "            overall_reward += reward\n",
    "\n",
    "            if done: reward = -5000 # punish if agent dies\n",
    "                \n",
    "            update_q_table(q_table, state, action, next_state, reward, alpha, gamma)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        if game % 100 == 0 and game != 0:\n",
    "            print('Episode:', game,  \n",
    "                  'Mean-Reward:', np.mean(last100_rewards), \n",
    "                  'Max-Reward:', max(last100_rewards),\n",
    "                  'Alpha:', alpha                     \n",
    "                 )\n",
    "            game_max.append(max(last100_rewards))\n",
    "            game_mean.append(np.mean(last100_rewards))\n",
    "            \n",
    "        if (np.mean(last100_rewards) >= 195) and not solved: \n",
    "            print('TASK COMPLETED LAST 100 GAMES HAD AN AVERAGE SCORE >=195 ON GAME', game)\n",
    "            print(last100_rewards)\n",
    "            solved = True\n",
    "                       \n",
    "        \n",
    "        last100_rewards.append(overall_reward) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "361.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "716px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
