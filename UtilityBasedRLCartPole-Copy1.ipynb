{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Greedy Utility-based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import operator\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env._max_episode_steps = 5000\n",
    "ACTION_SPACE = env.action_space.n # number of possible actions\n",
    "OBSERVATION_SPACE = env.observation_space.shape[0] # number of observable variables\n",
    "EXPLORATION_PROB = 1.0 # EPSILON GREEDY STRATEGY\n",
    "EXPLORATION_DECAY = 0.9995\n",
    "EXPLORATION_STOP = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_intervals():\n",
    "    intervals = np.zeros((OBSERVATION_SPACE, STATES_IN_INTERVAL))\n",
    "    intervals[0] = np.linspace(-4.8, 4.8, STATES_IN_INTERVAL)\n",
    "    intervals[1] = np.linspace(-3.5, 3.5, STATES_IN_INTERVAL)\n",
    "    intervals[2] = np.linspace(-0.42, 0.42, STATES_IN_INTERVAL)\n",
    "    intervals[3] = np.linspace(-4, 4, STATES_IN_INTERVAL)\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_observation(observation):\n",
    "    discrete_observation = np.array([np.digitize(observation[index], INTERVALS[index])-1 for index in range(OBSERVATION_SPACE)])\n",
    "    # if some value is under the lower border ignore it and give it min value\n",
    "    discrete_observation = [0 if x<0 else x for x in discrete_observation]\n",
    "    return discrete_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_possible_states():\n",
    "    digits = len(str(STATES_IN_INTERVAL))\n",
    "    state_indices = [str(state_index).zfill(digits) for state_index in range(STATES_IN_INTERVAL)] # all encodings for a single observation variable\n",
    "    states = [state_indices for i in range(OBSERVATION_SPACE)] # for each observation variable a list of its encodings\n",
    "    states = list(itertools.product(*states)) # get all permutation of all state encodings (->list of tuples)\n",
    "    states = [''.join(x) for x in states] # join tuples to a single string\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_to_state(observation):\n",
    "    discrete_observation = discretize_observation(observation)\n",
    "    digits = len(str(STATES_IN_INTERVAL))\n",
    "    \n",
    "    state = ''\n",
    "    for state_id in discrete_observation:\n",
    "        if len(str(state_id)) < digits:\n",
    "            state += str(state_id).zfill(digits)\n",
    "        else:\n",
    "            state += str(state_id)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env._max_episode_steps = 5000\n",
    "ACTION_SPACE = env.action_space.n #number of possible actions\n",
    "OBSERVATION_SPACE = env.observation_space.shape[0] #number of observable variables\n",
    "STATES_IN_INTERVAL = 10\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "EXPLORATION_PROB = 1.0 # EPSILON GREEDY STRATEGY\n",
    "EXPLORATION_DECAY = 0.995\n",
    "EXPLORATION_STOP = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_utility_table():\n",
    "    states = get_all_possible_states()\n",
    "    #utility_values = np.zeros(len(states))\n",
    "    utility_values = np.empty(len(states))\n",
    "    utility_values.fill(100)\n",
    "    utility_table = dict(zip(states, utility_values))\n",
    "    return utility_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reward_table():\n",
    "    states = get_all_possible_states()\n",
    "    rewards = np.zeros(len(states)) # init with zero; high; random\n",
    "    reward_table = dict(zip(states, rewards))\n",
    "    return reward_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_utility_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_number_state_action_table(nsa_table, state, action):\n",
    "#     key = (state, action)\n",
    "#     if key in nsa_table.keys():\n",
    "#         nsa_table[key] += 1\n",
    "#     else:\n",
    "#         nsa_table[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_number_state_action_next_state_table(nsas_table, state, action, next_state):\n",
    "    if (state, action) in nsas_table.keys():\n",
    "        if next_state in nsas_table[(state, action)].keys():\n",
    "            nsas_table[(state, action)][next_state] += 1\n",
    "        else:\n",
    "            nsas_table[(state, action)][next_state] = 1\n",
    "    else:\n",
    "        nsas_table[(state, action)] = {}\n",
    "        nsas_table[(state, action)][next_state] = 1\n",
    "        \n",
    "    return nsas_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_probability(nsas_table, state, action):\n",
    "    next_states = nsas_table[(state, action)]\n",
    "    temp = {}\n",
    "    for next_state in next_states:\n",
    "        temp[next_state] = nsas_table[(state, action)][next_state]/sum(nsas_table[(state, action)].values())\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nsa(nsas_table, state, action):\n",
    "    return sum(nsas_table[(state, action)].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_utility_estimate(utility_table, nsas_table, state, action, next_state, reward_table, epsilon, gamma):\n",
    "    next_states = nsas_table[(state, action)].keys()\n",
    "    u = 0\n",
    "    probs = get_transition_probability(nsas_table, state, action)\n",
    "    for next_state in next_states:\n",
    "        u +=  probs[next_state] * utility_table[next_state] ##\n",
    "    \n",
    "    actions = [0, 1]\n",
    "    f_values = []\n",
    "    if (state, actions[0]) in nsas_table.keys():\n",
    "        f_values.append(exploration_function(u, get_nsa(nsas_table, state, action), epsilon))\n",
    "    if (state, actions[1]) in nsas_table.keys():\n",
    "        f_values.append(exploration_function(u, get_nsa(nsas_table, state, action), epsilon))\n",
    "    if not f_values:\n",
    "        print('(O.O) we have a problem')\n",
    "    \n",
    "    utility_table[state] = reward_table[state] + gamma * max(f_values)\n",
    "    return utility_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_function(utility, n, epsilon):\n",
    "    if n < epsilon:\n",
    "        return 110\n",
    "    else:\n",
    "        return utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(utility_table, nsas_table, state): \n",
    "    best_a0, best_a1 = -100, -100\n",
    "    if (state, 0) in nsas_table.keys():\n",
    "        next_states_a0 = nsas_table[(state, 0)].keys()\n",
    "        best_a0 = max([utility_table[s] for s in next_states_a0])\n",
    "    if (state, 1) in nsas_table.keys():\n",
    "        next_states_a1 = nsas_table[(state, 1)].keys()\n",
    "        best_a1 = max([utility_table[s] for s in next_states_a1])\n",
    "    \n",
    "    if best_a0 == best_a1:\n",
    "        return randint(0,1)\n",
    "    \n",
    "    return (0 if best_a0 > best_a1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_states(nsas_table):\n",
    "    count = 0\n",
    "    for k, v in nsas_table.items():\n",
    "        for k1, v1 in v.items():\n",
    "            count +=1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bernhard\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\bernhard\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100 Epsilon: 0.603 Mean-Reward: 115.1 Max-Reward: 179.0 Explored: 801\n",
      "Episode: 200 Epsilon: 0.365 Mean-Reward: 133.95 Max-Reward: 189.0 Explored: 1013\n",
      "Episode: 300 Epsilon: 0.221 Mean-Reward: 129.54 Max-Reward: 173.0 Explored: 1115\n",
      "Episode: 400 Epsilon: 0.134 Mean-Reward: 126.89 Max-Reward: 261.0 Explored: 1157\n",
      "Episode: 500 Epsilon: 0.081 Mean-Reward: 130.03 Max-Reward: 186.0 Explored: 1232\n",
      "Episode: 600 Epsilon: 0.049 Mean-Reward: 124.56 Max-Reward: 175.0 Explored: 1266\n",
      "Episode: 700 Epsilon: 0.03 Mean-Reward: 127.13 Max-Reward: 228.0 Explored: 1346\n",
      "Episode: 800 Epsilon: 0.018 Mean-Reward: 118.42 Max-Reward: 185.0 Explored: 1383\n",
      "Episode: 900 Epsilon: 0.011 Mean-Reward: 121.25 Max-Reward: 198.0 Explored: 1531\n",
      "Episode: 1000 Epsilon: 0.01 Mean-Reward: 121.02 Max-Reward: 400.0 Explored: 1942\n",
      "TASK COMPLETED LAST 100 GAMES HAD AN AVERAGE SCORE >=195 ON GAME 1080\n",
      "deque([155.0, 39.0, 64.0, 136.0, 122.0, 192.0, 72.0, 50.0, 61.0, 131.0, 160.0, 109.0, 400.0, 181.0, 223.0, 177.0, 96.0, 173.0, 278.0, 239.0, 133.0, 231.0, 214.0, 208.0, 273.0, 232.0, 183.0, 176.0, 283.0, 303.0, 220.0, 199.0, 97.0, 174.0, 267.0, 218.0, 273.0, 147.0, 159.0, 217.0, 310.0, 175.0, 207.0, 200.0, 78.0, 473.0, 132.0, 130.0, 203.0, 185.0, 179.0, 113.0, 181.0, 358.0, 140.0, 107.0, 201.0, 132.0, 139.0, 188.0, 167.0, 126.0, 139.0, 170.0, 187.0, 140.0, 161.0, 189.0, 197.0, 142.0, 181.0, 257.0, 239.0, 216.0, 132.0, 244.0, 260.0, 162.0, 169.0, 148.0, 677.0, 113.0, 201.0, 251.0, 245.0, 199.0, 132.0, 237.0, 354.0, 232.0, 255.0, 102.0, 294.0, 191.0, 181.0, 276.0, 176.0, 243.0, 191.0, 241.0], maxlen=100)\n",
      "Episode: 1100 Epsilon: 0.01 Mean-Reward: 202.18 Max-Reward: 677.0 Explored: 2575\n",
      "Episode: 1200 Epsilon: 0.01 Mean-Reward: 184.66 Max-Reward: 547.0 Explored: 2797\n",
      "Episode: 1300 Epsilon: 0.01 Mean-Reward: 181.59 Max-Reward: 475.0 Explored: 3019\n",
      "Episode: 1400 Epsilon: 0.01 Mean-Reward: 159.7 Max-Reward: 497.0 Explored: 3103\n",
      "Episode: 1500 Epsilon: 0.01 Mean-Reward: 133.81 Max-Reward: 205.0 Explored: 3127\n",
      "Episode: 1600 Epsilon: 0.01 Mean-Reward: 179.6 Max-Reward: 650.0 Explored: 3267\n",
      "Episode: 1700 Epsilon: 0.01 Mean-Reward: 175.36 Max-Reward: 409.0 Explored: 3374\n",
      "Episode: 1800 Epsilon: 0.01 Mean-Reward: 149.71 Max-Reward: 393.0 Explored: 3435\n",
      "Episode: 1900 Epsilon: 0.01 Mean-Reward: 169.08 Max-Reward: 1260.0 Explored: 3505\n",
      "Episode: 2000 Epsilon: 0.01 Mean-Reward: 168.52 Max-Reward: 547.0 Explored: 3586\n",
      "Episode: 2100 Epsilon: 0.01 Mean-Reward: 172.67 Max-Reward: 539.0 Explored: 3624\n",
      "Episode: 2200 Epsilon: 0.01 Mean-Reward: 183.6 Max-Reward: 604.0 Explored: 3678\n",
      "Episode: 2300 Epsilon: 0.01 Mean-Reward: 168.18 Max-Reward: 473.0 Explored: 3704\n",
      "Episode: 2400 Epsilon: 0.01 Mean-Reward: 171.01 Max-Reward: 774.0 Explored: 3759\n",
      "Episode: 2500 Epsilon: 0.01 Mean-Reward: 193.41 Max-Reward: 804.0 Explored: 3846\n",
      "Episode: 2600 Epsilon: 0.01 Mean-Reward: 150.7 Max-Reward: 1100.0 Explored: 3864\n",
      "Episode: 2700 Epsilon: 0.01 Mean-Reward: 192.2 Max-Reward: 683.0 Explored: 3906\n",
      "Episode: 2800 Epsilon: 0.01 Mean-Reward: 246.09 Max-Reward: 1171.0 Explored: 3989\n",
      "Episode: 2900 Epsilon: 0.01 Mean-Reward: 187.58 Max-Reward: 1052.0 Explored: 4047\n",
      "Episode: 3000 Epsilon: 0.01 Mean-Reward: 163.4 Max-Reward: 623.0 Explored: 4091\n",
      "Episode: 3100 Epsilon: 0.01 Mean-Reward: 164.57 Max-Reward: 861.0 Explored: 4127\n",
      "Episode: 3200 Epsilon: 0.01 Mean-Reward: 185.8 Max-Reward: 443.0 Explored: 4187\n",
      "Episode: 3300 Epsilon: 0.01 Mean-Reward: 163.56 Max-Reward: 1085.0 Explored: 4202\n",
      "Episode: 3400 Epsilon: 0.01 Mean-Reward: 158.06 Max-Reward: 539.0 Explored: 4235\n",
      "Episode: 3500 Epsilon: 0.01 Mean-Reward: 160.24 Max-Reward: 589.0 Explored: 4261\n",
      "Episode: 3600 Epsilon: 0.01 Mean-Reward: 148.73 Max-Reward: 629.0 Explored: 4276\n",
      "Episode: 3700 Epsilon: 0.01 Mean-Reward: 161.77 Max-Reward: 441.0 Explored: 4289\n"
     ]
    }
   ],
   "source": [
    "INTERVALS = create_state_intervals()\n",
    "number_of_games = 200000\n",
    "\n",
    "\n",
    "#data = pd.DataFrame(columns = ['mean', 'max', 'solve_step'])\n",
    "data_max = []\n",
    "data_mean = []\n",
    "data_solve = []\n",
    "\n",
    "solved = False\n",
    "\n",
    "for i in range(1):\n",
    "    print('EPISODE:', i)\n",
    "        # init things we dont understand\n",
    "    utility_table = create_utility_table()\n",
    "    nsas_table = {}\n",
    "    reward_table = create_reward_table()\n",
    "\n",
    "    gamma = 0.8\n",
    "    explore = 1 ## change this (jahrers idea ... critical)\n",
    "    epsilon = EXPLORATION_PROB\n",
    "    \n",
    "    if i != 0:\n",
    "        data_max.append(game_max)\n",
    "        data_mean.append(game_mean)\n",
    "        if not solved:\n",
    "            data_solve.append(-1)\n",
    "    last100_rewards = deque(maxlen=100) # fifo queue\n",
    "    game_max = []\n",
    "    game_mean = []\n",
    "    solved = False\n",
    "    for game in range(number_of_games):\n",
    "        overall_reward, done = 0, False\n",
    "        observation = env.reset()\n",
    "        state = observation_to_state(observation)\n",
    "\n",
    "        while not done:\n",
    "            if game % 1000 == 0: env.render()\n",
    "            #if np.random.rand() < epsilon:\n",
    "            #    action = env.action_space.sample()\n",
    "            #else:\n",
    "            action = get_action(utility_table, nsas_table, state)\n",
    "\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            next_state = observation_to_state(observation)\n",
    "            overall_reward += reward\n",
    "\n",
    "            if done: reward = -5000 # punish if agent dies\n",
    "\n",
    "            reward_table[state] = reward\n",
    "            nsas_table = update_number_state_action_next_state_table(nsas_table, state, action, next_state)\n",
    "\n",
    "            utility_table = update_utility_estimate(utility_table, nsas_table, state, action, next_state, reward_table, explore, gamma)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        if epsilon > EXPLORATION_STOP: epsilon *= EXPLORATION_DECAY\n",
    "\n",
    "        if game % 100 == 0 and game != 0:\n",
    "            print('Episode:', game, 'Epsilon:', round(epsilon, 3), \n",
    "                  'Mean-Reward:', np.mean(last100_rewards), 'Max-Reward:', max(last100_rewards),\n",
    "                 'Explored:', count_states(nsas_table))\n",
    "            \n",
    "        if (np.mean(last100_rewards) >= 195) and not solved: \n",
    "            print('TASK COMPLETED LAST 100 GAMES HAD AN AVERAGE SCORE >=195 ON GAME', game)\n",
    "            print(last100_rewards)\n",
    "            solved = True\n",
    "            data_solve.append(game)\n",
    "            \n",
    "        if game % 100 == 0 and game != 0:\n",
    "            game_max.append(max(last100_rewards))\n",
    "            game_mean.append(np.mean(last100_rewards))\n",
    "        \n",
    "        last100_rewards.append(overall_reward) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
