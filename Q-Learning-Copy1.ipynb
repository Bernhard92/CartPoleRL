{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Dependent Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import gym \n",
    "import operator\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env._max_episode_steps = 5000\n",
    "number_of_games = 10000000\n",
    "ACTION_SPACE = env.action_space.n #number of possible actions\n",
    "OBSERVATION_SPACE = env.observation_space.shape[0] #number of observable variables\n",
    "STATES_IN_INTERVAL = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def create_state_intervals():\n",
    "    intervals = np.zeros((OBSERVATION_SPACE, STATES_IN_INTERVAL))\n",
    "    intervals[0] = np.linspace(-4.8, 4.8, STATES_IN_INTERVAL)\n",
    "    intervals[1] = np.linspace(-3.5, 3.5, STATES_IN_INTERVAL)\n",
    "    intervals[2] = np.linspace(-0.42, 0.42, STATES_IN_INTERVAL)\n",
    "    intervals[3] = np.linspace(-4, 4, STATES_IN_INTERVAL)\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def discretize_observation(observation):\n",
    "    discrete_observation = np.array([np.digitize(observation[index], INTERVALS[index])-1 for index in range(OBSERVATION_SPACE)])\n",
    "    # if some value is under the lower border ignore it and give it min value\n",
    "    discrete_observation = [0 if x<0 else x for x in discrete_observation]\n",
    "    return discrete_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_all_possible_states():\n",
    "    digits = len(str(STATES_IN_INTERVAL))\n",
    "    state_indices = [str(state_index).zfill(digits) for state_index in range(STATES_IN_INTERVAL)] # all encodings for a single observation variable\n",
    "    states = [state_indices for i in range(OBSERVATION_SPACE)] # for each observation variable a list of its encodings\n",
    "    states = list(itertools.product(*states)) # get all permutation of all state encodings (->list of tuples)\n",
    "    states = [''.join(x) for x in states] # join tuples to a single string\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def observation_to_state(observation):\n",
    "    discrete_observation = discretize_observation(observation)\n",
    "    digits = len(str(STATES_IN_INTERVAL))\n",
    "    \n",
    "    state = ''\n",
    "    for state_id in discrete_observation:\n",
    "        if len(str(state_id)) < digits:\n",
    "            state += str(state_id).zfill(digits)\n",
    "        else:\n",
    "            state += str(state_id)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_q_table(states, actions):\n",
    "    q_table = dict()\n",
    "    for state in states:\n",
    "        q_table[state] = dict()\n",
    "        for action in actions:\n",
    "            q_table[state][action] = np.random.randint(10)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(q_table, state, alpha):\n",
    "    action = 0 if q_table[state][0] > q_table[state][1] else 1\n",
    "    if(random.random() < alpha):\n",
    "        action += 1 \n",
    "        action %= 2\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_a(q_table, next_state):\n",
    "    return max(q_table[next_state][k] for k in q_table[next_state].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(q_table, state, action, next_state, reward, alpha, gamma):\n",
    "    q_s_a = q_table[state][action]\n",
    "    q_table[state][action] = q_s_a + alpha * (reward + gamma * max_a(q_table, next_state) - q_s_a)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bejahrer\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\bejahrer\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100 Mean-Reward: 15.28 Max-Reward: 35.0 Alpha: 0.9038873549665959\n",
      "Episode: 200 Mean-Reward: 12.55 Max-Reward: 38.0 Alpha: 0.8178301806491574\n",
      "Episode: 300 Mean-Reward: 13.16 Max-Reward: 40.0 Alpha: 0.7399663251239436\n",
      "Episode: 400 Mean-Reward: 15.28 Max-Reward: 61.0 Alpha: 0.6695157201007336\n",
      "Episode: 500 Mean-Reward: 16.97 Max-Reward: 55.0 Alpha: 0.6057725659163237\n",
      "Episode: 600 Mean-Reward: 17.14 Max-Reward: 72.0 Alpha: 0.548098260578011\n",
      "Episode: 700 Mean-Reward: 22.04 Max-Reward: 62.0 Alpha: 0.4959150020176678\n",
      "Episode: 800 Mean-Reward: 24.77 Max-Reward: 69.0 Alpha: 0.44869999946146477\n",
      "Episode: 900 Mean-Reward: 30.46 Max-Reward: 85.0 Alpha: 0.4059802359226587\n",
      "Episode: 1000 Mean-Reward: 35.96 Max-Reward: 168.0 Alpha: 0.36732772934619257\n",
      "Episode: 1100 Mean-Reward: 40.5 Max-Reward: 123.0 Alpha: 0.33235524492954527\n",
      "Episode: 1200 Mean-Reward: 48.28 Max-Reward: 158.0 Alpha: 0.3007124156643058\n",
      "Episode: 1300 Mean-Reward: 55.03 Max-Reward: 128.0 Alpha: 0.2720822322326576\n",
      "Episode: 1400 Mean-Reward: 68.82 Max-Reward: 178.0 Alpha: 0.2461778670932771\n",
      "Episode: 1500 Mean-Reward: 85.37 Max-Reward: 206.0 Alpha: 0.22273980093919937\n",
      "Episode: 1600 Mean-Reward: 94.49 Max-Reward: 293.0 Alpha: 0.2015332227394583\n",
      "Episode: 1700 Mean-Reward: 92.72 Max-Reward: 320.0 Alpha: 0.18234567731717977\n",
      "Episode: 1800 Mean-Reward: 124.52 Max-Reward: 246.0 Alpha: 0.1649849368967147\n",
      "Episode: 1900 Mean-Reward: 150.4 Max-Reward: 304.0 Alpha: 0.14927707529619813\n",
      "Episode: 2000 Mean-Reward: 148.92 Max-Reward: 321.0 Alpha: 0.13506472547210188\n",
      "Episode: 2100 Mean-Reward: 164.23 Max-Reward: 393.0 Alpha: 0.12220550295922675\n",
      "Episode: 2200 Mean-Reward: 168.64 Max-Reward: 305.0 Alpha: 0.11057057941158951\n",
      "Episode: 2300 Mean-Reward: 176.5 Max-Reward: 428.0 Alpha: 0.10004339195341891\n",
      "Episode: 2400 Mean-Reward: 180.05 Max-Reward: 336.0 Alpha: 0.09051847541007228\n",
      "TASK COMPLETED LAST 100 GAMES HAD AN AVERAGE SCORE >=195 ON GAME 2454\n",
      "deque([186.0, 175.0, 170.0, 120.0, 155.0, 218.0, 165.0, 144.0, 141.0, 136.0, 168.0, 237.0, 184.0, 237.0, 164.0, 172.0, 159.0, 189.0, 190.0, 148.0, 168.0, 176.0, 207.0, 182.0, 182.0, 202.0, 135.0, 172.0, 123.0, 188.0, 152.0, 213.0, 240.0, 204.0, 234.0, 187.0, 241.0, 162.0, 208.0, 125.0, 336.0, 214.0, 139.0, 251.0, 110.0, 160.0, 168.0, 183.0, 276.0, 280.0, 365.0, 498.0, 196.0, 189.0, 234.0, 153.0, 174.0, 165.0, 198.0, 206.0, 56.0, 282.0, 190.0, 189.0, 170.0, 215.0, 162.0, 233.0, 197.0, 144.0, 149.0, 182.0, 158.0, 177.0, 234.0, 188.0, 208.0, 149.0, 185.0, 164.0, 95.0, 128.0, 163.0, 200.0, 150.0, 164.0, 171.0, 168.0, 261.0, 219.0, 161.0, 175.0, 186.0, 139.0, 167.0, 169.0, 369.0, 399.0, 260.0, 501.0], maxlen=100)\n",
      "Episode: 2500 Mean-Reward: 215.26 Max-Reward: 501.0 Alpha: 0.08190040571973876\n",
      "Episode: 2600 Mean-Reward: 200.33 Max-Reward: 366.0 Alpha: 0.07410284394064628\n",
      "Episode: 2700 Mean-Reward: 194.18 Max-Reward: 433.0 Alpha: 0.06704767127628951\n",
      "Episode: 2800 Mean-Reward: 192.59 Max-Reward: 308.0 Alpha: 0.060664206453048174\n",
      "Episode: 2900 Mean-Reward: 191.33 Max-Reward: 424.0 Alpha: 0.05488849760960279\n",
      "Episode: 3000 Mean-Reward: 200.02 Max-Reward: 358.0 Alpha: 0.049662681604038215\n",
      "Episode: 3100 Mean-Reward: 190.12 Max-Reward: 382.0 Alpha: 0.04493440431994225\n",
      "Episode: 3200 Mean-Reward: 208.19 Max-Reward: 328.0 Alpha: 0.04065629616391608\n",
      "Episode: 3300 Mean-Reward: 207.52 Max-Reward: 408.0 Alpha: 0.03678549749984046\n",
      "Episode: 3400 Mean-Reward: 194.61 Max-Reward: 292.0 Alpha: 0.03328322926552661\n",
      "Episode: 3500 Mean-Reward: 222.1 Max-Reward: 375.0 Alpha: 0.030114404470033673\n",
      "Episode: 3600 Mean-Reward: 232.86 Max-Reward: 343.0 Alpha: 0.027247276679492435\n",
      "Episode: 3700 Mean-Reward: 220.69 Max-Reward: 330.0 Alpha: 0.024653121969839265\n",
      "Episode: 3800 Mean-Reward: 201.82 Max-Reward: 455.0 Alpha: 0.022305951160147018\n",
      "Episode: 3900 Mean-Reward: 202.2 Max-Reward: 361.0 Alpha: 0.02018224944360293\n",
      "Episode: 4000 Mean-Reward: 231.04 Max-Reward: 432.0 Alpha: 0.018260740807661956\n",
      "Episode: 4100 Mean-Reward: 243.21 Max-Reward: 370.0 Alpha: 0.016522174883251375\n",
      "Episode: 4200 Mean-Reward: 233.73 Max-Reward: 328.0 Alpha: 0.014949134087605212\n",
      "Episode: 4300 Mean-Reward: 233.54 Max-Reward: 361.0 Alpha: 0.01352585912861506\n",
      "Episode: 4400 Mean-Reward: 230.41 Max-Reward: 380.0 Alpha: 0.012238091122537187\n",
      "Episode: 4500 Mean-Reward: 245.88 Max-Reward: 396.0 Alpha: 0.011072928743333644\n",
      "Episode: 4600 Mean-Reward: 243.26 Max-Reward: 380.0 Alpha: 0.010018698972517958\n",
      "Episode: 4700 Mean-Reward: 238.17 Max-Reward: 385.0 Alpha: 0.009998671593271896\n",
      "Episode: 4800 Mean-Reward: 250.99 Max-Reward: 438.0 Alpha: 0.009998671593271896\n",
      "Episode: 4900 Mean-Reward: 240.38 Max-Reward: 464.0 Alpha: 0.009998671593271896\n",
      "Episode: 5000 Mean-Reward: 235.38 Max-Reward: 356.0 Alpha: 0.009998671593271896\n",
      "Episode: 5100 Mean-Reward: 237.71 Max-Reward: 358.0 Alpha: 0.009998671593271896\n",
      "Episode: 5200 Mean-Reward: 243.5 Max-Reward: 445.0 Alpha: 0.009998671593271896\n",
      "Episode: 5300 Mean-Reward: 255.74 Max-Reward: 442.0 Alpha: 0.009998671593271896\n",
      "Episode: 5400 Mean-Reward: 253.0 Max-Reward: 472.0 Alpha: 0.009998671593271896\n",
      "Episode: 5500 Mean-Reward: 251.34 Max-Reward: 426.0 Alpha: 0.009998671593271896\n",
      "Episode: 5600 Mean-Reward: 248.56 Max-Reward: 375.0 Alpha: 0.009998671593271896\n",
      "Episode: 5700 Mean-Reward: 243.41 Max-Reward: 385.0 Alpha: 0.009998671593271896\n",
      "Episode: 5800 Mean-Reward: 241.97 Max-Reward: 401.0 Alpha: 0.009998671593271896\n",
      "Episode: 5900 Mean-Reward: 233.1 Max-Reward: 381.0 Alpha: 0.009998671593271896\n",
      "Episode: 6000 Mean-Reward: 235.46 Max-Reward: 387.0 Alpha: 0.009998671593271896\n",
      "Episode: 6100 Mean-Reward: 232.25 Max-Reward: 335.0 Alpha: 0.009998671593271896\n",
      "Episode: 6200 Mean-Reward: 233.91 Max-Reward: 391.0 Alpha: 0.009998671593271896\n",
      "Episode: 6300 Mean-Reward: 233.0 Max-Reward: 348.0 Alpha: 0.009998671593271896\n",
      "Episode: 6400 Mean-Reward: 232.06 Max-Reward: 361.0 Alpha: 0.009998671593271896\n",
      "Episode: 6500 Mean-Reward: 228.74 Max-Reward: 421.0 Alpha: 0.009998671593271896\n",
      "Episode: 6600 Mean-Reward: 228.09 Max-Reward: 388.0 Alpha: 0.009998671593271896\n",
      "Episode: 6700 Mean-Reward: 234.91 Max-Reward: 371.0 Alpha: 0.009998671593271896\n",
      "Episode: 6800 Mean-Reward: 234.65 Max-Reward: 382.0 Alpha: 0.009998671593271896\n",
      "Episode: 6900 Mean-Reward: 237.81 Max-Reward: 432.0 Alpha: 0.009998671593271896\n",
      "Episode: 7000 Mean-Reward: 235.98 Max-Reward: 342.0 Alpha: 0.009998671593271896\n",
      "Episode: 7100 Mean-Reward: 220.21 Max-Reward: 297.0 Alpha: 0.009998671593271896\n",
      "Episode: 7200 Mean-Reward: 238.8 Max-Reward: 400.0 Alpha: 0.009998671593271896\n",
      "Episode: 7300 Mean-Reward: 228.4 Max-Reward: 366.0 Alpha: 0.009998671593271896\n",
      "Episode: 7400 Mean-Reward: 232.43 Max-Reward: 334.0 Alpha: 0.009998671593271896\n",
      "Episode: 7500 Mean-Reward: 235.97 Max-Reward: 398.0 Alpha: 0.009998671593271896\n",
      "Episode: 7600 Mean-Reward: 237.79 Max-Reward: 361.0 Alpha: 0.009998671593271896\n",
      "Episode: 7700 Mean-Reward: 232.76 Max-Reward: 390.0 Alpha: 0.009998671593271896\n",
      "Episode: 7800 Mean-Reward: 238.39 Max-Reward: 401.0 Alpha: 0.009998671593271896\n",
      "Episode: 7900 Mean-Reward: 223.88 Max-Reward: 462.0 Alpha: 0.009998671593271896\n",
      "Episode: 8000 Mean-Reward: 232.37 Max-Reward: 400.0 Alpha: 0.009998671593271896\n",
      "Episode: 8100 Mean-Reward: 241.29 Max-Reward: 379.0 Alpha: 0.009998671593271896\n",
      "Episode: 8200 Mean-Reward: 236.84 Max-Reward: 396.0 Alpha: 0.009998671593271896\n",
      "Episode: 8300 Mean-Reward: 233.74 Max-Reward: 514.0 Alpha: 0.009998671593271896\n",
      "Episode: 8400 Mean-Reward: 226.33 Max-Reward: 369.0 Alpha: 0.009998671593271896\n",
      "Episode: 8500 Mean-Reward: 234.21 Max-Reward: 440.0 Alpha: 0.009998671593271896\n",
      "Episode: 8600 Mean-Reward: 229.84 Max-Reward: 334.0 Alpha: 0.009998671593271896\n",
      "Episode: 8700 Mean-Reward: 238.64 Max-Reward: 587.0 Alpha: 0.009998671593271896\n",
      "Episode: 8800 Mean-Reward: 228.47 Max-Reward: 358.0 Alpha: 0.009998671593271896\n",
      "Episode: 8900 Mean-Reward: 222.84 Max-Reward: 312.0 Alpha: 0.009998671593271896\n",
      "Episode: 9000 Mean-Reward: 227.71 Max-Reward: 341.0 Alpha: 0.009998671593271896\n",
      "Episode: 9100 Mean-Reward: 222.79 Max-Reward: 315.0 Alpha: 0.009998671593271896\n",
      "Episode: 9200 Mean-Reward: 227.44 Max-Reward: 411.0 Alpha: 0.009998671593271896\n",
      "Episode: 9300 Mean-Reward: 226.73 Max-Reward: 578.0 Alpha: 0.009998671593271896\n",
      "Episode: 9400 Mean-Reward: 226.64 Max-Reward: 403.0 Alpha: 0.009998671593271896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 9500 Mean-Reward: 220.67 Max-Reward: 378.0 Alpha: 0.009998671593271896\n",
      "Episode: 9600 Mean-Reward: 232.43 Max-Reward: 443.0 Alpha: 0.009998671593271896\n",
      "Episode: 9700 Mean-Reward: 231.69 Max-Reward: 337.0 Alpha: 0.009998671593271896\n",
      "Episode: 9800 Mean-Reward: 228.45 Max-Reward: 419.0 Alpha: 0.009998671593271896\n",
      "Episode: 9900 Mean-Reward: 233.09 Max-Reward: 451.0 Alpha: 0.009998671593271896\n",
      "Episode: 10000 Mean-Reward: 251.94 Max-Reward: 431.0 Alpha: 0.009998671593271896\n",
      "Episode: 10100 Mean-Reward: 252.7 Max-Reward: 392.0 Alpha: 0.009998671593271896\n",
      "Episode: 10200 Mean-Reward: 256.56 Max-Reward: 404.0 Alpha: 0.009998671593271896\n",
      "Episode: 10300 Mean-Reward: 246.57 Max-Reward: 346.0 Alpha: 0.009998671593271896\n",
      "Episode: 10400 Mean-Reward: 250.1 Max-Reward: 426.0 Alpha: 0.009998671593271896\n",
      "Episode: 10500 Mean-Reward: 238.59 Max-Reward: 407.0 Alpha: 0.009998671593271896\n",
      "Episode: 10600 Mean-Reward: 228.29 Max-Reward: 332.0 Alpha: 0.009998671593271896\n",
      "Episode: 10700 Mean-Reward: 241.01 Max-Reward: 377.0 Alpha: 0.009998671593271896\n",
      "Episode: 10800 Mean-Reward: 247.39 Max-Reward: 409.0 Alpha: 0.009998671593271896\n",
      "Episode: 10900 Mean-Reward: 236.8 Max-Reward: 325.0 Alpha: 0.009998671593271896\n",
      "Episode: 11000 Mean-Reward: 251.96 Max-Reward: 426.0 Alpha: 0.009998671593271896\n",
      "Episode: 11100 Mean-Reward: 229.62 Max-Reward: 593.0 Alpha: 0.009998671593271896\n",
      "Episode: 11200 Mean-Reward: 237.78 Max-Reward: 362.0 Alpha: 0.009998671593271896\n",
      "Episode: 11300 Mean-Reward: 222.03 Max-Reward: 378.0 Alpha: 0.009998671593271896\n",
      "Episode: 11400 Mean-Reward: 243.1 Max-Reward: 414.0 Alpha: 0.009998671593271896\n",
      "Episode: 11500 Mean-Reward: 244.39 Max-Reward: 368.0 Alpha: 0.009998671593271896\n",
      "Episode: 11600 Mean-Reward: 262.22 Max-Reward: 448.0 Alpha: 0.009998671593271896\n",
      "Episode: 11700 Mean-Reward: 255.01 Max-Reward: 515.0 Alpha: 0.009998671593271896\n",
      "Episode: 11800 Mean-Reward: 286.71 Max-Reward: 582.0 Alpha: 0.009998671593271896\n",
      "Episode: 11900 Mean-Reward: 222.35 Max-Reward: 486.0 Alpha: 0.009998671593271896\n",
      "Episode: 12000 Mean-Reward: 217.01 Max-Reward: 399.0 Alpha: 0.009998671593271896\n",
      "Episode: 12100 Mean-Reward: 250.34 Max-Reward: 401.0 Alpha: 0.009998671593271896\n",
      "Episode: 12200 Mean-Reward: 211.29 Max-Reward: 341.0 Alpha: 0.009998671593271896\n",
      "Episode: 12300 Mean-Reward: 256.06 Max-Reward: 405.0 Alpha: 0.009998671593271896\n",
      "Episode: 12400 Mean-Reward: 266.1 Max-Reward: 558.0 Alpha: 0.009998671593271896\n",
      "Episode: 12500 Mean-Reward: 260.73 Max-Reward: 423.0 Alpha: 0.009998671593271896\n",
      "Episode: 12600 Mean-Reward: 265.1 Max-Reward: 486.0 Alpha: 0.009998671593271896\n",
      "Episode: 12700 Mean-Reward: 270.5 Max-Reward: 543.0 Alpha: 0.009998671593271896\n",
      "Episode: 12800 Mean-Reward: 300.4 Max-Reward: 480.0 Alpha: 0.009998671593271896\n",
      "Episode: 12900 Mean-Reward: 309.12 Max-Reward: 530.0 Alpha: 0.009998671593271896\n",
      "Episode: 13000 Mean-Reward: 318.1 Max-Reward: 481.0 Alpha: 0.009998671593271896\n",
      "Episode: 13100 Mean-Reward: 323.9 Max-Reward: 685.0 Alpha: 0.009998671593271896\n",
      "Episode: 13200 Mean-Reward: 302.47 Max-Reward: 441.0 Alpha: 0.009998671593271896\n",
      "Episode: 13300 Mean-Reward: 301.27 Max-Reward: 450.0 Alpha: 0.009998671593271896\n",
      "Episode: 13400 Mean-Reward: 316.57 Max-Reward: 526.0 Alpha: 0.009998671593271896\n",
      "Episode: 13500 Mean-Reward: 308.83 Max-Reward: 549.0 Alpha: 0.009998671593271896\n",
      "Episode: 13600 Mean-Reward: 324.0 Max-Reward: 682.0 Alpha: 0.009998671593271896\n",
      "Episode: 13700 Mean-Reward: 331.59 Max-Reward: 569.0 Alpha: 0.009998671593271896\n",
      "Episode: 13800 Mean-Reward: 317.67 Max-Reward: 701.0 Alpha: 0.009998671593271896\n",
      "Episode: 13900 Mean-Reward: 312.14 Max-Reward: 476.0 Alpha: 0.009998671593271896\n",
      "Episode: 14000 Mean-Reward: 325.17 Max-Reward: 505.0 Alpha: 0.009998671593271896\n",
      "Episode: 14100 Mean-Reward: 321.69 Max-Reward: 548.0 Alpha: 0.009998671593271896\n",
      "Episode: 14200 Mean-Reward: 326.37 Max-Reward: 494.0 Alpha: 0.009998671593271896\n",
      "Episode: 14300 Mean-Reward: 322.65 Max-Reward: 495.0 Alpha: 0.009998671593271896\n",
      "Episode: 14400 Mean-Reward: 333.02 Max-Reward: 563.0 Alpha: 0.009998671593271896\n",
      "Episode: 14500 Mean-Reward: 325.68 Max-Reward: 796.0 Alpha: 0.009998671593271896\n",
      "Episode: 14600 Mean-Reward: 326.33 Max-Reward: 610.0 Alpha: 0.009998671593271896\n",
      "Episode: 14700 Mean-Reward: 322.9 Max-Reward: 527.0 Alpha: 0.009998671593271896\n",
      "Episode: 14800 Mean-Reward: 343.04 Max-Reward: 632.0 Alpha: 0.009998671593271896\n",
      "Episode: 14900 Mean-Reward: 350.24 Max-Reward: 810.0 Alpha: 0.009998671593271896\n",
      "Episode: 15000 Mean-Reward: 341.2 Max-Reward: 620.0 Alpha: 0.009998671593271896\n",
      "Episode: 15100 Mean-Reward: 337.04 Max-Reward: 758.0 Alpha: 0.009998671593271896\n",
      "Episode: 15200 Mean-Reward: 329.97 Max-Reward: 787.0 Alpha: 0.009998671593271896\n",
      "Episode: 15300 Mean-Reward: 337.31 Max-Reward: 472.0 Alpha: 0.009998671593271896\n",
      "Episode: 15400 Mean-Reward: 316.74 Max-Reward: 459.0 Alpha: 0.009998671593271896\n",
      "Episode: 15500 Mean-Reward: 344.77 Max-Reward: 578.0 Alpha: 0.009998671593271896\n",
      "Episode: 15600 Mean-Reward: 262.54 Max-Reward: 569.0 Alpha: 0.009998671593271896\n",
      "Episode: 15700 Mean-Reward: 327.03 Max-Reward: 488.0 Alpha: 0.009998671593271896\n",
      "Episode: 15800 Mean-Reward: 314.35 Max-Reward: 722.0 Alpha: 0.009998671593271896\n",
      "Episode: 15900 Mean-Reward: 324.67 Max-Reward: 594.0 Alpha: 0.009998671593271896\n",
      "Episode: 16000 Mean-Reward: 322.05 Max-Reward: 741.0 Alpha: 0.009998671593271896\n",
      "Episode: 16100 Mean-Reward: 320.17 Max-Reward: 479.0 Alpha: 0.009998671593271896\n",
      "Episode: 16200 Mean-Reward: 311.05 Max-Reward: 625.0 Alpha: 0.009998671593271896\n",
      "Episode: 16300 Mean-Reward: 306.71 Max-Reward: 625.0 Alpha: 0.009998671593271896\n",
      "Episode: 16400 Mean-Reward: 316.46 Max-Reward: 513.0 Alpha: 0.009998671593271896\n",
      "Episode: 16500 Mean-Reward: 307.79 Max-Reward: 518.0 Alpha: 0.009998671593271896\n",
      "Episode: 16600 Mean-Reward: 301.4 Max-Reward: 513.0 Alpha: 0.009998671593271896\n",
      "Episode: 16700 Mean-Reward: 309.76 Max-Reward: 661.0 Alpha: 0.009998671593271896\n",
      "Episode: 16800 Mean-Reward: 318.31 Max-Reward: 564.0 Alpha: 0.009998671593271896\n",
      "Episode: 16900 Mean-Reward: 325.37 Max-Reward: 533.0 Alpha: 0.009998671593271896\n",
      "Episode: 17000 Mean-Reward: 340.7 Max-Reward: 806.0 Alpha: 0.009998671593271896\n",
      "Episode: 17100 Mean-Reward: 334.85 Max-Reward: 666.0 Alpha: 0.009998671593271896\n",
      "Episode: 17200 Mean-Reward: 343.44 Max-Reward: 557.0 Alpha: 0.009998671593271896\n",
      "Episode: 17300 Mean-Reward: 349.79 Max-Reward: 559.0 Alpha: 0.009998671593271896\n",
      "Episode: 17400 Mean-Reward: 337.3 Max-Reward: 622.0 Alpha: 0.009998671593271896\n",
      "Episode: 17500 Mean-Reward: 337.2 Max-Reward: 723.0 Alpha: 0.009998671593271896\n",
      "Episode: 17600 Mean-Reward: 339.55 Max-Reward: 489.0 Alpha: 0.009998671593271896\n",
      "Episode: 17700 Mean-Reward: 362.9 Max-Reward: 693.0 Alpha: 0.009998671593271896\n",
      "Episode: 17800 Mean-Reward: 343.28 Max-Reward: 657.0 Alpha: 0.009998671593271896\n",
      "Episode: 17900 Mean-Reward: 330.11 Max-Reward: 506.0 Alpha: 0.009998671593271896\n",
      "Episode: 18000 Mean-Reward: 343.7 Max-Reward: 673.0 Alpha: 0.009998671593271896\n",
      "Episode: 18100 Mean-Reward: 342.18 Max-Reward: 712.0 Alpha: 0.009998671593271896\n",
      "Episode: 18200 Mean-Reward: 339.22 Max-Reward: 568.0 Alpha: 0.009998671593271896\n",
      "Episode: 18300 Mean-Reward: 333.09 Max-Reward: 883.0 Alpha: 0.009998671593271896\n",
      "Episode: 18400 Mean-Reward: 346.36 Max-Reward: 740.0 Alpha: 0.009998671593271896\n",
      "Episode: 18500 Mean-Reward: 345.1 Max-Reward: 538.0 Alpha: 0.009998671593271896\n",
      "Episode: 18600 Mean-Reward: 339.64 Max-Reward: 564.0 Alpha: 0.009998671593271896\n",
      "Episode: 18700 Mean-Reward: 355.51 Max-Reward: 665.0 Alpha: 0.009998671593271896\n",
      "Episode: 18800 Mean-Reward: 351.37 Max-Reward: 656.0 Alpha: 0.009998671593271896\n",
      "Episode: 18900 Mean-Reward: 354.51 Max-Reward: 611.0 Alpha: 0.009998671593271896\n",
      "Episode: 19000 Mean-Reward: 331.68 Max-Reward: 517.0 Alpha: 0.009998671593271896\n",
      "Episode: 19100 Mean-Reward: 352.98 Max-Reward: 766.0 Alpha: 0.009998671593271896\n",
      "Episode: 19200 Mean-Reward: 348.17 Max-Reward: 517.0 Alpha: 0.009998671593271896\n",
      "Episode: 19300 Mean-Reward: 342.56 Max-Reward: 687.0 Alpha: 0.009998671593271896\n",
      "Episode: 19400 Mean-Reward: 358.48 Max-Reward: 555.0 Alpha: 0.009998671593271896\n",
      "Episode: 19500 Mean-Reward: 337.89 Max-Reward: 499.0 Alpha: 0.009998671593271896\n",
      "Episode: 19600 Mean-Reward: 330.64 Max-Reward: 609.0 Alpha: 0.009998671593271896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 19700 Mean-Reward: 345.26 Max-Reward: 797.0 Alpha: 0.009998671593271896\n",
      "Episode: 19800 Mean-Reward: 336.55 Max-Reward: 439.0 Alpha: 0.009998671593271896\n",
      "Episode: 19900 Mean-Reward: 337.98 Max-Reward: 788.0 Alpha: 0.009998671593271896\n",
      "Episode: 20000 Mean-Reward: 339.43 Max-Reward: 572.0 Alpha: 0.009998671593271896\n",
      "Episode: 20100 Mean-Reward: 319.19 Max-Reward: 529.0 Alpha: 0.009998671593271896\n",
      "Episode: 20200 Mean-Reward: 349.61 Max-Reward: 613.0 Alpha: 0.009998671593271896\n",
      "Episode: 20300 Mean-Reward: 338.02 Max-Reward: 678.0 Alpha: 0.009998671593271896\n",
      "Episode: 20400 Mean-Reward: 354.18 Max-Reward: 604.0 Alpha: 0.009998671593271896\n",
      "Episode: 20500 Mean-Reward: 353.92 Max-Reward: 755.0 Alpha: 0.009998671593271896\n",
      "Episode: 20600 Mean-Reward: 340.26 Max-Reward: 594.0 Alpha: 0.009998671593271896\n",
      "Episode: 20700 Mean-Reward: 337.01 Max-Reward: 558.0 Alpha: 0.009998671593271896\n",
      "Episode: 20800 Mean-Reward: 344.0 Max-Reward: 751.0 Alpha: 0.009998671593271896\n",
      "Episode: 20900 Mean-Reward: 340.57 Max-Reward: 620.0 Alpha: 0.009998671593271896\n",
      "Episode: 21000 Mean-Reward: 346.09 Max-Reward: 751.0 Alpha: 0.009998671593271896\n",
      "Episode: 21100 Mean-Reward: 342.71 Max-Reward: 535.0 Alpha: 0.009998671593271896\n",
      "Episode: 21200 Mean-Reward: 340.4 Max-Reward: 634.0 Alpha: 0.009998671593271896\n",
      "Episode: 21300 Mean-Reward: 343.35 Max-Reward: 590.0 Alpha: 0.009998671593271896\n",
      "Episode: 21400 Mean-Reward: 335.19 Max-Reward: 550.0 Alpha: 0.009998671593271896\n",
      "Episode: 21500 Mean-Reward: 321.29 Max-Reward: 457.0 Alpha: 0.009998671593271896\n",
      "Episode: 21600 Mean-Reward: 338.17 Max-Reward: 506.0 Alpha: 0.009998671593271896\n",
      "Episode: 21700 Mean-Reward: 347.01 Max-Reward: 671.0 Alpha: 0.009998671593271896\n",
      "Episode: 21800 Mean-Reward: 351.99 Max-Reward: 594.0 Alpha: 0.009998671593271896\n",
      "Episode: 21900 Mean-Reward: 368.14 Max-Reward: 588.0 Alpha: 0.009998671593271896\n",
      "Episode: 22000 Mean-Reward: 378.76 Max-Reward: 608.0 Alpha: 0.009998671593271896\n",
      "Episode: 22100 Mean-Reward: 365.79 Max-Reward: 835.0 Alpha: 0.009998671593271896\n",
      "Episode: 22200 Mean-Reward: 376.85 Max-Reward: 579.0 Alpha: 0.009998671593271896\n",
      "Episode: 22300 Mean-Reward: 370.96 Max-Reward: 615.0 Alpha: 0.009998671593271896\n",
      "Episode: 22400 Mean-Reward: 379.19 Max-Reward: 614.0 Alpha: 0.009998671593271896\n",
      "Episode: 22500 Mean-Reward: 382.81 Max-Reward: 727.0 Alpha: 0.009998671593271896\n",
      "Episode: 22600 Mean-Reward: 407.01 Max-Reward: 918.0 Alpha: 0.009998671593271896\n",
      "Episode: 22700 Mean-Reward: 387.32 Max-Reward: 810.0 Alpha: 0.009998671593271896\n",
      "Episode: 22800 Mean-Reward: 397.07 Max-Reward: 630.0 Alpha: 0.009998671593271896\n",
      "Episode: 22900 Mean-Reward: 386.72 Max-Reward: 711.0 Alpha: 0.009998671593271896\n",
      "Episode: 23000 Mean-Reward: 367.45 Max-Reward: 647.0 Alpha: 0.009998671593271896\n",
      "Episode: 23100 Mean-Reward: 367.31 Max-Reward: 610.0 Alpha: 0.009998671593271896\n",
      "Episode: 23200 Mean-Reward: 360.27 Max-Reward: 573.0 Alpha: 0.009998671593271896\n",
      "Episode: 23300 Mean-Reward: 346.67 Max-Reward: 544.0 Alpha: 0.009998671593271896\n",
      "Episode: 23400 Mean-Reward: 389.83 Max-Reward: 843.0 Alpha: 0.009998671593271896\n",
      "Episode: 23500 Mean-Reward: 414.28 Max-Reward: 676.0 Alpha: 0.009998671593271896\n",
      "Episode: 23600 Mean-Reward: 285.37 Max-Reward: 637.0 Alpha: 0.009998671593271896\n",
      "Episode: 23700 Mean-Reward: 356.74 Max-Reward: 921.0 Alpha: 0.009998671593271896\n",
      "Episode: 23800 Mean-Reward: 398.31 Max-Reward: 627.0 Alpha: 0.009998671593271896\n",
      "Episode: 23900 Mean-Reward: 405.33 Max-Reward: 871.0 Alpha: 0.009998671593271896\n",
      "Episode: 24000 Mean-Reward: 395.44 Max-Reward: 586.0 Alpha: 0.009998671593271896\n",
      "Episode: 24100 Mean-Reward: 381.29 Max-Reward: 612.0 Alpha: 0.009998671593271896\n",
      "Episode: 24200 Mean-Reward: 377.02 Max-Reward: 554.0 Alpha: 0.009998671593271896\n",
      "Episode: 24300 Mean-Reward: 365.13 Max-Reward: 559.0 Alpha: 0.009998671593271896\n",
      "Episode: 24400 Mean-Reward: 369.35 Max-Reward: 619.0 Alpha: 0.009998671593271896\n",
      "Episode: 24500 Mean-Reward: 388.43 Max-Reward: 665.0 Alpha: 0.009998671593271896\n",
      "Episode: 24600 Mean-Reward: 379.55 Max-Reward: 605.0 Alpha: 0.009998671593271896\n",
      "Episode: 24700 Mean-Reward: 403.77 Max-Reward: 725.0 Alpha: 0.009998671593271896\n",
      "Episode: 24800 Mean-Reward: 376.5 Max-Reward: 725.0 Alpha: 0.009998671593271896\n",
      "Episode: 24900 Mean-Reward: 296.07 Max-Reward: 832.0 Alpha: 0.009998671593271896\n",
      "Episode: 25000 Mean-Reward: 388.72 Max-Reward: 669.0 Alpha: 0.009998671593271896\n",
      "Episode: 25100 Mean-Reward: 353.96 Max-Reward: 679.0 Alpha: 0.009998671593271896\n",
      "Episode: 25200 Mean-Reward: 342.39 Max-Reward: 617.0 Alpha: 0.009998671593271896\n",
      "Episode: 25300 Mean-Reward: 343.48 Max-Reward: 584.0 Alpha: 0.009998671593271896\n",
      "Episode: 25400 Mean-Reward: 360.83 Max-Reward: 672.0 Alpha: 0.009998671593271896\n",
      "Episode: 25500 Mean-Reward: 339.83 Max-Reward: 515.0 Alpha: 0.009998671593271896\n",
      "Episode: 25600 Mean-Reward: 338.29 Max-Reward: 548.0 Alpha: 0.009998671593271896\n",
      "Episode: 25700 Mean-Reward: 339.18 Max-Reward: 704.0 Alpha: 0.009998671593271896\n",
      "Episode: 25800 Mean-Reward: 331.46 Max-Reward: 577.0 Alpha: 0.009998671593271896\n",
      "Episode: 25900 Mean-Reward: 325.15 Max-Reward: 537.0 Alpha: 0.009998671593271896\n",
      "Episode: 26000 Mean-Reward: 330.69 Max-Reward: 586.0 Alpha: 0.009998671593271896\n",
      "Episode: 26100 Mean-Reward: 352.81 Max-Reward: 609.0 Alpha: 0.009998671593271896\n",
      "Episode: 26200 Mean-Reward: 348.07 Max-Reward: 572.0 Alpha: 0.009998671593271896\n",
      "Episode: 26300 Mean-Reward: 364.68 Max-Reward: 573.0 Alpha: 0.009998671593271896\n",
      "Episode: 26400 Mean-Reward: 299.9 Max-Reward: 574.0 Alpha: 0.009998671593271896\n",
      "Episode: 26500 Mean-Reward: 302.76 Max-Reward: 916.0 Alpha: 0.009998671593271896\n",
      "Episode: 26600 Mean-Reward: 309.7 Max-Reward: 554.0 Alpha: 0.009998671593271896\n",
      "Episode: 26700 Mean-Reward: 286.92 Max-Reward: 548.0 Alpha: 0.009998671593271896\n",
      "Episode: 26800 Mean-Reward: 269.53 Max-Reward: 765.0 Alpha: 0.009998671593271896\n",
      "Episode: 26900 Mean-Reward: 269.09 Max-Reward: 588.0 Alpha: 0.009998671593271896\n",
      "Episode: 27000 Mean-Reward: 289.04 Max-Reward: 682.0 Alpha: 0.009998671593271896\n",
      "Episode: 27100 Mean-Reward: 328.29 Max-Reward: 600.0 Alpha: 0.009998671593271896\n",
      "Episode: 27200 Mean-Reward: 304.14 Max-Reward: 668.0 Alpha: 0.009998671593271896\n",
      "Episode: 27300 Mean-Reward: 327.3 Max-Reward: 570.0 Alpha: 0.009998671593271896\n",
      "Episode: 27400 Mean-Reward: 302.28 Max-Reward: 611.0 Alpha: 0.009998671593271896\n",
      "Episode: 27500 Mean-Reward: 307.22 Max-Reward: 598.0 Alpha: 0.009998671593271896\n",
      "Episode: 27600 Mean-Reward: 311.55 Max-Reward: 700.0 Alpha: 0.009998671593271896\n",
      "Episode: 27700 Mean-Reward: 339.09 Max-Reward: 657.0 Alpha: 0.009998671593271896\n",
      "Episode: 27800 Mean-Reward: 320.7 Max-Reward: 717.0 Alpha: 0.009998671593271896\n",
      "Episode: 27900 Mean-Reward: 330.44 Max-Reward: 675.0 Alpha: 0.009998671593271896\n",
      "Episode: 28000 Mean-Reward: 341.73 Max-Reward: 602.0 Alpha: 0.009998671593271896\n",
      "Episode: 28100 Mean-Reward: 357.01 Max-Reward: 932.0 Alpha: 0.009998671593271896\n",
      "Episode: 28200 Mean-Reward: 336.02 Max-Reward: 694.0 Alpha: 0.009998671593271896\n",
      "Episode: 28300 Mean-Reward: 302.23 Max-Reward: 811.0 Alpha: 0.009998671593271896\n",
      "Episode: 28400 Mean-Reward: 273.59 Max-Reward: 456.0 Alpha: 0.009998671593271896\n",
      "Episode: 28500 Mean-Reward: 288.98 Max-Reward: 521.0 Alpha: 0.009998671593271896\n",
      "Episode: 28600 Mean-Reward: 334.16 Max-Reward: 900.0 Alpha: 0.009998671593271896\n",
      "Episode: 28700 Mean-Reward: 313.1 Max-Reward: 777.0 Alpha: 0.009998671593271896\n",
      "Episode: 28800 Mean-Reward: 290.89 Max-Reward: 563.0 Alpha: 0.009998671593271896\n",
      "Episode: 28900 Mean-Reward: 321.17 Max-Reward: 689.0 Alpha: 0.009998671593271896\n",
      "Episode: 29000 Mean-Reward: 269.09 Max-Reward: 498.0 Alpha: 0.009998671593271896\n",
      "Episode: 29100 Mean-Reward: 256.41 Max-Reward: 464.0 Alpha: 0.009998671593271896\n",
      "Episode: 29200 Mean-Reward: 253.9 Max-Reward: 451.0 Alpha: 0.009998671593271896\n",
      "Episode: 29300 Mean-Reward: 288.38 Max-Reward: 561.0 Alpha: 0.009998671593271896\n",
      "Episode: 29400 Mean-Reward: 276.13 Max-Reward: 558.0 Alpha: 0.009998671593271896\n",
      "Episode: 29500 Mean-Reward: 288.54 Max-Reward: 711.0 Alpha: 0.009998671593271896\n",
      "Episode: 29600 Mean-Reward: 306.5 Max-Reward: 558.0 Alpha: 0.009998671593271896\n",
      "Episode: 29700 Mean-Reward: 328.52 Max-Reward: 698.0 Alpha: 0.009998671593271896\n",
      "Episode: 29800 Mean-Reward: 305.58 Max-Reward: 539.0 Alpha: 0.009998671593271896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 29900 Mean-Reward: 289.14 Max-Reward: 587.0 Alpha: 0.009998671593271896\n",
      "Episode: 30000 Mean-Reward: 260.77 Max-Reward: 422.0 Alpha: 0.009998671593271896\n",
      "Episode: 30100 Mean-Reward: 271.27 Max-Reward: 443.0 Alpha: 0.009998671593271896\n",
      "Episode: 30200 Mean-Reward: 275.59 Max-Reward: 450.0 Alpha: 0.009998671593271896\n",
      "Episode: 30300 Mean-Reward: 281.68 Max-Reward: 426.0 Alpha: 0.009998671593271896\n",
      "Episode: 30400 Mean-Reward: 239.45 Max-Reward: 353.0 Alpha: 0.009998671593271896\n",
      "Episode: 30500 Mean-Reward: 263.1 Max-Reward: 431.0 Alpha: 0.009998671593271896\n",
      "Episode: 30600 Mean-Reward: 290.14 Max-Reward: 448.0 Alpha: 0.009998671593271896\n",
      "Episode: 30700 Mean-Reward: 286.04 Max-Reward: 539.0 Alpha: 0.009998671593271896\n",
      "Episode: 30800 Mean-Reward: 283.38 Max-Reward: 552.0 Alpha: 0.009998671593271896\n",
      "Episode: 30900 Mean-Reward: 295.5 Max-Reward: 705.0 Alpha: 0.009998671593271896\n",
      "Episode: 31000 Mean-Reward: 284.53 Max-Reward: 796.0 Alpha: 0.009998671593271896\n",
      "Episode: 31100 Mean-Reward: 279.01 Max-Reward: 483.0 Alpha: 0.009998671593271896\n",
      "Episode: 31200 Mean-Reward: 297.82 Max-Reward: 801.0 Alpha: 0.009998671593271896\n",
      "Episode: 31300 Mean-Reward: 262.53 Max-Reward: 456.0 Alpha: 0.009998671593271896\n",
      "Episode: 31400 Mean-Reward: 291.44 Max-Reward: 702.0 Alpha: 0.009998671593271896\n",
      "Episode: 31500 Mean-Reward: 287.4 Max-Reward: 476.0 Alpha: 0.009998671593271896\n",
      "Episode: 31600 Mean-Reward: 289.57 Max-Reward: 520.0 Alpha: 0.009998671593271896\n",
      "Episode: 31700 Mean-Reward: 281.34 Max-Reward: 536.0 Alpha: 0.009998671593271896\n",
      "Episode: 31800 Mean-Reward: 286.91 Max-Reward: 475.0 Alpha: 0.009998671593271896\n",
      "Episode: 31900 Mean-Reward: 274.85 Max-Reward: 534.0 Alpha: 0.009998671593271896\n",
      "Episode: 32000 Mean-Reward: 284.08 Max-Reward: 495.0 Alpha: 0.009998671593271896\n",
      "Episode: 32100 Mean-Reward: 297.21 Max-Reward: 585.0 Alpha: 0.009998671593271896\n",
      "Episode: 32200 Mean-Reward: 266.71 Max-Reward: 432.0 Alpha: 0.009998671593271896\n",
      "Episode: 32300 Mean-Reward: 286.79 Max-Reward: 466.0 Alpha: 0.009998671593271896\n",
      "Episode: 32400 Mean-Reward: 284.63 Max-Reward: 557.0 Alpha: 0.009998671593271896\n",
      "Episode: 32500 Mean-Reward: 289.01 Max-Reward: 733.0 Alpha: 0.009998671593271896\n",
      "Episode: 32600 Mean-Reward: 291.24 Max-Reward: 708.0 Alpha: 0.009998671593271896\n",
      "Episode: 32700 Mean-Reward: 282.37 Max-Reward: 599.0 Alpha: 0.009998671593271896\n",
      "Episode: 32800 Mean-Reward: 290.35 Max-Reward: 459.0 Alpha: 0.009998671593271896\n",
      "Episode: 32900 Mean-Reward: 281.51 Max-Reward: 466.0 Alpha: 0.009998671593271896\n",
      "Episode: 33000 Mean-Reward: 294.32 Max-Reward: 489.0 Alpha: 0.009998671593271896\n",
      "Episode: 33100 Mean-Reward: 267.03 Max-Reward: 545.0 Alpha: 0.009998671593271896\n",
      "Episode: 33200 Mean-Reward: 302.21 Max-Reward: 714.0 Alpha: 0.009998671593271896\n",
      "Episode: 33300 Mean-Reward: 286.41 Max-Reward: 535.0 Alpha: 0.009998671593271896\n",
      "Episode: 33400 Mean-Reward: 284.91 Max-Reward: 654.0 Alpha: 0.009998671593271896\n",
      "Episode: 33500 Mean-Reward: 285.56 Max-Reward: 536.0 Alpha: 0.009998671593271896\n",
      "Episode: 33600 Mean-Reward: 277.27 Max-Reward: 635.0 Alpha: 0.009998671593271896\n",
      "Episode: 33700 Mean-Reward: 268.73 Max-Reward: 659.0 Alpha: 0.009998671593271896\n",
      "Episode: 33800 Mean-Reward: 275.97 Max-Reward: 714.0 Alpha: 0.009998671593271896\n",
      "Episode: 33900 Mean-Reward: 286.87 Max-Reward: 480.0 Alpha: 0.009998671593271896\n",
      "Episode: 34000 Mean-Reward: 285.76 Max-Reward: 559.0 Alpha: 0.009998671593271896\n",
      "Episode: 34100 Mean-Reward: 274.5 Max-Reward: 732.0 Alpha: 0.009998671593271896\n",
      "Episode: 34200 Mean-Reward: 263.24 Max-Reward: 587.0 Alpha: 0.009998671593271896\n",
      "Episode: 34300 Mean-Reward: 281.05 Max-Reward: 546.0 Alpha: 0.009998671593271896\n",
      "Episode: 34400 Mean-Reward: 282.6 Max-Reward: 666.0 Alpha: 0.009998671593271896\n",
      "Episode: 34500 Mean-Reward: 311.9 Max-Reward: 630.0 Alpha: 0.009998671593271896\n",
      "Episode: 34600 Mean-Reward: 331.95 Max-Reward: 557.0 Alpha: 0.009998671593271896\n",
      "Episode: 34700 Mean-Reward: 298.24 Max-Reward: 621.0 Alpha: 0.009998671593271896\n",
      "Episode: 34800 Mean-Reward: 313.31 Max-Reward: 563.0 Alpha: 0.009998671593271896\n",
      "Episode: 34900 Mean-Reward: 347.43 Max-Reward: 655.0 Alpha: 0.009998671593271896\n",
      "Episode: 35000 Mean-Reward: 351.08 Max-Reward: 632.0 Alpha: 0.009998671593271896\n",
      "Episode: 35100 Mean-Reward: 362.5 Max-Reward: 666.0 Alpha: 0.009998671593271896\n",
      "Episode: 35200 Mean-Reward: 358.28 Max-Reward: 677.0 Alpha: 0.009998671593271896\n",
      "Episode: 35300 Mean-Reward: 356.91 Max-Reward: 678.0 Alpha: 0.009998671593271896\n",
      "Episode: 35400 Mean-Reward: 367.14 Max-Reward: 771.0 Alpha: 0.009998671593271896\n",
      "Episode: 35500 Mean-Reward: 364.33 Max-Reward: 813.0 Alpha: 0.009998671593271896\n",
      "Episode: 35600 Mean-Reward: 356.01 Max-Reward: 710.0 Alpha: 0.009998671593271896\n",
      "Episode: 35700 Mean-Reward: 352.51 Max-Reward: 836.0 Alpha: 0.009998671593271896\n",
      "Episode: 35800 Mean-Reward: 414.3 Max-Reward: 937.0 Alpha: 0.009998671593271896\n",
      "Episode: 35900 Mean-Reward: 291.6 Max-Reward: 776.0 Alpha: 0.009998671593271896\n",
      "Episode: 36000 Mean-Reward: 278.66 Max-Reward: 461.0 Alpha: 0.009998671593271896\n",
      "Episode: 36100 Mean-Reward: 258.92 Max-Reward: 391.0 Alpha: 0.009998671593271896\n",
      "Episode: 36200 Mean-Reward: 259.2 Max-Reward: 428.0 Alpha: 0.009998671593271896\n",
      "Episode: 36300 Mean-Reward: 264.85 Max-Reward: 414.0 Alpha: 0.009998671593271896\n",
      "Episode: 36400 Mean-Reward: 279.89 Max-Reward: 540.0 Alpha: 0.009998671593271896\n",
      "Episode: 36500 Mean-Reward: 260.75 Max-Reward: 504.0 Alpha: 0.009998671593271896\n",
      "Episode: 36600 Mean-Reward: 261.78 Max-Reward: 468.0 Alpha: 0.009998671593271896\n",
      "Episode: 36700 Mean-Reward: 282.51 Max-Reward: 494.0 Alpha: 0.009998671593271896\n",
      "Episode: 36800 Mean-Reward: 272.91 Max-Reward: 557.0 Alpha: 0.009998671593271896\n",
      "Episode: 36900 Mean-Reward: 279.92 Max-Reward: 543.0 Alpha: 0.009998671593271896\n",
      "Episode: 37000 Mean-Reward: 278.1 Max-Reward: 532.0 Alpha: 0.009998671593271896\n",
      "Episode: 37100 Mean-Reward: 270.08 Max-Reward: 470.0 Alpha: 0.009998671593271896\n",
      "Episode: 37200 Mean-Reward: 285.19 Max-Reward: 561.0 Alpha: 0.009998671593271896\n",
      "Episode: 37300 Mean-Reward: 272.37 Max-Reward: 493.0 Alpha: 0.009998671593271896\n",
      "Episode: 37400 Mean-Reward: 289.9 Max-Reward: 485.0 Alpha: 0.009998671593271896\n",
      "Episode: 37500 Mean-Reward: 278.2 Max-Reward: 470.0 Alpha: 0.009998671593271896\n",
      "Episode: 37600 Mean-Reward: 276.92 Max-Reward: 551.0 Alpha: 0.009998671593271896\n",
      "Episode: 37700 Mean-Reward: 255.52 Max-Reward: 441.0 Alpha: 0.009998671593271896\n",
      "Episode: 37800 Mean-Reward: 260.11 Max-Reward: 606.0 Alpha: 0.009998671593271896\n",
      "Episode: 37900 Mean-Reward: 282.72 Max-Reward: 478.0 Alpha: 0.009998671593271896\n",
      "Episode: 38000 Mean-Reward: 284.56 Max-Reward: 557.0 Alpha: 0.009998671593271896\n",
      "Episode: 38100 Mean-Reward: 281.02 Max-Reward: 514.0 Alpha: 0.009998671593271896\n",
      "Episode: 38200 Mean-Reward: 285.51 Max-Reward: 510.0 Alpha: 0.009998671593271896\n",
      "Episode: 38300 Mean-Reward: 289.89 Max-Reward: 583.0 Alpha: 0.009998671593271896\n",
      "Episode: 38400 Mean-Reward: 275.75 Max-Reward: 478.0 Alpha: 0.009998671593271896\n",
      "Episode: 38500 Mean-Reward: 267.68 Max-Reward: 500.0 Alpha: 0.009998671593271896\n",
      "Episode: 38600 Mean-Reward: 269.46 Max-Reward: 448.0 Alpha: 0.009998671593271896\n",
      "Episode: 38700 Mean-Reward: 271.24 Max-Reward: 525.0 Alpha: 0.009998671593271896\n",
      "Episode: 38800 Mean-Reward: 293.31 Max-Reward: 515.0 Alpha: 0.009998671593271896\n",
      "Episode: 38900 Mean-Reward: 280.51 Max-Reward: 422.0 Alpha: 0.009998671593271896\n",
      "Episode: 39000 Mean-Reward: 290.17 Max-Reward: 455.0 Alpha: 0.009998671593271896\n",
      "Episode: 39100 Mean-Reward: 282.87 Max-Reward: 419.0 Alpha: 0.009998671593271896\n",
      "Episode: 39200 Mean-Reward: 276.98 Max-Reward: 478.0 Alpha: 0.009998671593271896\n",
      "Episode: 39300 Mean-Reward: 255.95 Max-Reward: 532.0 Alpha: 0.009998671593271896\n",
      "Episode: 39400 Mean-Reward: 295.48 Max-Reward: 489.0 Alpha: 0.009998671593271896\n",
      "Episode: 39500 Mean-Reward: 299.61 Max-Reward: 578.0 Alpha: 0.009998671593271896\n",
      "Episode: 39600 Mean-Reward: 305.45 Max-Reward: 586.0 Alpha: 0.009998671593271896\n",
      "Episode: 39700 Mean-Reward: 286.41 Max-Reward: 585.0 Alpha: 0.009998671593271896\n",
      "Episode: 39800 Mean-Reward: 377.08 Max-Reward: 636.0 Alpha: 0.009998671593271896\n",
      "Episode: 39900 Mean-Reward: 311.28 Max-Reward: 638.0 Alpha: 0.009998671593271896\n",
      "Episode: 40000 Mean-Reward: 287.96 Max-Reward: 504.0 Alpha: 0.009998671593271896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 40100 Mean-Reward: 287.21 Max-Reward: 510.0 Alpha: 0.009998671593271896\n",
      "Episode: 40200 Mean-Reward: 279.35 Max-Reward: 571.0 Alpha: 0.009998671593271896\n",
      "Episode: 40300 Mean-Reward: 294.67 Max-Reward: 684.0 Alpha: 0.009998671593271896\n",
      "Episode: 40400 Mean-Reward: 285.17 Max-Reward: 561.0 Alpha: 0.009998671593271896\n",
      "Episode: 40500 Mean-Reward: 290.33 Max-Reward: 468.0 Alpha: 0.009998671593271896\n",
      "Episode: 40600 Mean-Reward: 301.87 Max-Reward: 658.0 Alpha: 0.009998671593271896\n",
      "Episode: 40700 Mean-Reward: 293.63 Max-Reward: 780.0 Alpha: 0.009998671593271896\n",
      "Episode: 40800 Mean-Reward: 284.83 Max-Reward: 558.0 Alpha: 0.009998671593271896\n",
      "Episode: 40900 Mean-Reward: 312.57 Max-Reward: 591.0 Alpha: 0.009998671593271896\n",
      "Episode: 41000 Mean-Reward: 370.8 Max-Reward: 1109.0 Alpha: 0.009998671593271896\n",
      "Episode: 41100 Mean-Reward: 324.08 Max-Reward: 766.0 Alpha: 0.009998671593271896\n",
      "Episode: 41200 Mean-Reward: 380.42 Max-Reward: 1022.0 Alpha: 0.009998671593271896\n",
      "Episode: 41300 Mean-Reward: 371.25 Max-Reward: 724.0 Alpha: 0.009998671593271896\n",
      "Episode: 41400 Mean-Reward: 376.43 Max-Reward: 678.0 Alpha: 0.009998671593271896\n",
      "Episode: 41500 Mean-Reward: 368.01 Max-Reward: 839.0 Alpha: 0.009998671593271896\n",
      "Episode: 41600 Mean-Reward: 379.44 Max-Reward: 1187.0 Alpha: 0.009998671593271896\n",
      "Episode: 41700 Mean-Reward: 355.94 Max-Reward: 608.0 Alpha: 0.009998671593271896\n",
      "Episode: 41800 Mean-Reward: 351.33 Max-Reward: 591.0 Alpha: 0.009998671593271896\n",
      "Episode: 41900 Mean-Reward: 378.95 Max-Reward: 801.0 Alpha: 0.009998671593271896\n",
      "Episode: 42000 Mean-Reward: 380.83 Max-Reward: 811.0 Alpha: 0.009998671593271896\n",
      "Episode: 42100 Mean-Reward: 373.34 Max-Reward: 820.0 Alpha: 0.009998671593271896\n",
      "Episode: 42200 Mean-Reward: 379.22 Max-Reward: 663.0 Alpha: 0.009998671593271896\n",
      "Episode: 42300 Mean-Reward: 367.68 Max-Reward: 634.0 Alpha: 0.009998671593271896\n",
      "Episode: 42400 Mean-Reward: 384.99 Max-Reward: 1211.0 Alpha: 0.009998671593271896\n",
      "Episode: 42500 Mean-Reward: 391.14 Max-Reward: 789.0 Alpha: 0.009998671593271896\n",
      "Episode: 42600 Mean-Reward: 378.89 Max-Reward: 751.0 Alpha: 0.009998671593271896\n",
      "Episode: 42700 Mean-Reward: 389.02 Max-Reward: 725.0 Alpha: 0.009998671593271896\n",
      "Episode: 42800 Mean-Reward: 374.49 Max-Reward: 853.0 Alpha: 0.009998671593271896\n",
      "Episode: 42900 Mean-Reward: 410.47 Max-Reward: 1060.0 Alpha: 0.009998671593271896\n",
      "Episode: 43000 Mean-Reward: 412.62 Max-Reward: 929.0 Alpha: 0.009998671593271896\n",
      "Episode: 43100 Mean-Reward: 435.13 Max-Reward: 872.0 Alpha: 0.009998671593271896\n",
      "Episode: 43200 Mean-Reward: 435.5 Max-Reward: 1243.0 Alpha: 0.009998671593271896\n",
      "Episode: 43300 Mean-Reward: 471.14 Max-Reward: 1503.0 Alpha: 0.009998671593271896\n",
      "Episode: 43400 Mean-Reward: 434.14 Max-Reward: 1161.0 Alpha: 0.009998671593271896\n",
      "Episode: 43500 Mean-Reward: 464.34 Max-Reward: 997.0 Alpha: 0.009998671593271896\n",
      "Episode: 43600 Mean-Reward: 450.26 Max-Reward: 942.0 Alpha: 0.009998671593271896\n",
      "Episode: 43700 Mean-Reward: 445.79 Max-Reward: 1069.0 Alpha: 0.009998671593271896\n",
      "Episode: 43800 Mean-Reward: 454.87 Max-Reward: 913.0 Alpha: 0.009998671593271896\n",
      "Episode: 43900 Mean-Reward: 470.15 Max-Reward: 924.0 Alpha: 0.009998671593271896\n",
      "Episode: 44000 Mean-Reward: 458.72 Max-Reward: 1136.0 Alpha: 0.009998671593271896\n",
      "Episode: 44100 Mean-Reward: 452.53 Max-Reward: 945.0 Alpha: 0.009998671593271896\n",
      "Episode: 44200 Mean-Reward: 482.51 Max-Reward: 1341.0 Alpha: 0.009998671593271896\n",
      "Episode: 44300 Mean-Reward: 438.77 Max-Reward: 978.0 Alpha: 0.009998671593271896\n",
      "Episode: 44400 Mean-Reward: 459.1 Max-Reward: 1025.0 Alpha: 0.009998671593271896\n",
      "Episode: 44500 Mean-Reward: 446.02 Max-Reward: 955.0 Alpha: 0.009998671593271896\n",
      "Episode: 44600 Mean-Reward: 445.58 Max-Reward: 813.0 Alpha: 0.009998671593271896\n",
      "Episode: 44700 Mean-Reward: 460.78 Max-Reward: 1037.0 Alpha: 0.009998671593271896\n",
      "Episode: 44800 Mean-Reward: 491.16 Max-Reward: 1038.0 Alpha: 0.009998671593271896\n",
      "Episode: 44900 Mean-Reward: 472.18 Max-Reward: 1072.0 Alpha: 0.009998671593271896\n",
      "Episode: 45000 Mean-Reward: 470.17 Max-Reward: 1510.0 Alpha: 0.009998671593271896\n",
      "Episode: 45100 Mean-Reward: 438.85 Max-Reward: 1092.0 Alpha: 0.009998671593271896\n",
      "Episode: 45200 Mean-Reward: 457.97 Max-Reward: 936.0 Alpha: 0.009998671593271896\n",
      "Episode: 45300 Mean-Reward: 485.24 Max-Reward: 1195.0 Alpha: 0.009998671593271896\n",
      "Episode: 45400 Mean-Reward: 469.86 Max-Reward: 1010.0 Alpha: 0.009998671593271896\n",
      "Episode: 45500 Mean-Reward: 454.88 Max-Reward: 996.0 Alpha: 0.009998671593271896\n",
      "Episode: 45600 Mean-Reward: 482.95 Max-Reward: 1012.0 Alpha: 0.009998671593271896\n",
      "Episode: 45700 Mean-Reward: 507.67 Max-Reward: 1181.0 Alpha: 0.009998671593271896\n",
      "Episode: 45800 Mean-Reward: 501.03 Max-Reward: 1133.0 Alpha: 0.009998671593271896\n",
      "Episode: 45900 Mean-Reward: 566.93 Max-Reward: 1861.0 Alpha: 0.009998671593271896\n",
      "Episode: 46000 Mean-Reward: 570.8 Max-Reward: 1494.0 Alpha: 0.009998671593271896\n",
      "Episode: 46100 Mean-Reward: 562.78 Max-Reward: 2246.0 Alpha: 0.009998671593271896\n",
      "Episode: 46200 Mean-Reward: 562.97 Max-Reward: 1326.0 Alpha: 0.009998671593271896\n",
      "Episode: 46300 Mean-Reward: 544.45 Max-Reward: 1471.0 Alpha: 0.009998671593271896\n",
      "Episode: 46400 Mean-Reward: 592.4 Max-Reward: 2272.0 Alpha: 0.009998671593271896\n",
      "Episode: 46500 Mean-Reward: 552.77 Max-Reward: 1676.0 Alpha: 0.009998671593271896\n",
      "Episode: 46600 Mean-Reward: 558.9 Max-Reward: 1084.0 Alpha: 0.009998671593271896\n",
      "Episode: 46700 Mean-Reward: 592.31 Max-Reward: 2022.0 Alpha: 0.009998671593271896\n",
      "Episode: 46800 Mean-Reward: 559.76 Max-Reward: 1386.0 Alpha: 0.009998671593271896\n",
      "Episode: 46900 Mean-Reward: 580.74 Max-Reward: 1755.0 Alpha: 0.009998671593271896\n",
      "Episode: 47000 Mean-Reward: 526.33 Max-Reward: 1123.0 Alpha: 0.009998671593271896\n",
      "Episode: 47100 Mean-Reward: 564.49 Max-Reward: 1704.0 Alpha: 0.009998671593271896\n"
     ]
    }
   ],
   "source": [
    "INTERVALS = create_state_intervals()\n",
    "\n",
    "for i in range(1):\n",
    "    print('EPISODE:', i)\n",
    "            \n",
    "    last100_rewards = deque(maxlen=100) # fifo queue\n",
    "    game_max = []\n",
    "    game_mean = []\n",
    "    solved = False\n",
    "    \n",
    "    q_table = init_q_table(get_all_possible_states(), [0, 1])\n",
    "    \n",
    "    \n",
    "    alpha = 1\n",
    "    gamma = 0.9\n",
    "          \n",
    "    for game in range(number_of_games):\n",
    "        \n",
    "        overall_reward, done = 0, False\n",
    "        observation = env.reset()\n",
    "        state = observation_to_state(observation)\n",
    "        \n",
    "        if alpha > 0.01:\n",
    "            alpha *= 0.999\n",
    "            \n",
    "        while not done:\n",
    "            #if game % 1000 == 0: env.render()\n",
    "                       \n",
    "            action = get_action(q_table, state, alpha)\n",
    "\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            \n",
    "            next_state = observation_to_state(observation)\n",
    "            overall_reward += reward\n",
    "\n",
    "            if done: reward = -5000 # punish if agent dies\n",
    "                \n",
    "            update_q_table(q_table, state, action, next_state, reward, alpha, gamma)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        if game % 100 == 0 and game != 0:\n",
    "            print('Episode:', game,  \n",
    "                  'Mean-Reward:', np.mean(last100_rewards), \n",
    "                  'Max-Reward:', max(last100_rewards),\n",
    "                  'Alpha:', alpha                     \n",
    "                 )\n",
    "            game_max.append(max(last100_rewards))\n",
    "            game_mean.append(np.mean(last100_rewards))\n",
    "            \n",
    "        if (np.mean(last100_rewards) >= 195) and not solved: \n",
    "            print('TASK COMPLETED LAST 100 GAMES HAD AN AVERAGE SCORE >=195 ON GAME', game)\n",
    "            print(last100_rewards)\n",
    "            solved = True\n",
    "                       \n",
    "        \n",
    "        last100_rewards.append(overall_reward) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "361.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "716px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
