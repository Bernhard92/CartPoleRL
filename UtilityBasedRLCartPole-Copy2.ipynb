{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Greedy Utility-based Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import operator\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env._max_episode_steps = 5000\n",
    "ACTION_SPACE = env.action_space.n # number of possible actions\n",
    "OBSERVATION_SPACE = env.observation_space.shape[0] # number of observable variables\n",
    "EXPLORATION_PROB = 1.0 # EPSILON GREEDY STRATEGY\n",
    "EXPLORATION_DECAY = 0.9995\n",
    "EXPLORATION_STOP = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_intervals():\n",
    "    intervals = np.zeros((OBSERVATION_SPACE, STATES_IN_INTERVAL))\n",
    "    intervals[0] = np.linspace(-4.8, 4.8, STATES_IN_INTERVAL)\n",
    "    intervals[1] = np.linspace(-3.5, 3.5, STATES_IN_INTERVAL)\n",
    "    intervals[2] = np.linspace(-0.42, 0.42, STATES_IN_INTERVAL)\n",
    "    intervals[3] = np.linspace(-4, 4, STATES_IN_INTERVAL)\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_observation(observation):\n",
    "    discrete_observation = np.array([np.digitize(observation[index], INTERVALS[index])-1 for index in range(OBSERVATION_SPACE)])\n",
    "    # if some value is under the lower border ignore it and give it min value\n",
    "    discrete_observation = [0 if x<0 else x for x in discrete_observation]\n",
    "    return discrete_observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_possible_states():\n",
    "    digits = len(str(STATES_IN_INTERVAL))\n",
    "    state_indices = [str(state_index).zfill(digits) for state_index in range(STATES_IN_INTERVAL)] # all encodings for a single observation variable\n",
    "    states = [state_indices for i in range(OBSERVATION_SPACE)] # for each observation variable a list of its encodings\n",
    "    states = list(itertools.product(*states)) # get all permutation of all state encodings (->list of tuples)\n",
    "    states = [''.join(x) for x in states] # join tuples to a single string\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observation_to_state(observation):\n",
    "    discrete_observation = discretize_observation(observation)\n",
    "    digits = len(str(STATES_IN_INTERVAL))\n",
    "    \n",
    "    state = ''\n",
    "    for state_id in discrete_observation:\n",
    "        if len(str(state_id)) < digits:\n",
    "            state += str(state_id).zfill(digits)\n",
    "        else:\n",
    "            state += str(state_id)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env._max_episode_steps = 5000\n",
    "ACTION_SPACE = env.action_space.n #number of possible actions\n",
    "OBSERVATION_SPACE = env.observation_space.shape[0] #number of observable variables\n",
    "STATES_IN_INTERVAL = 10\n",
    "#LEARNING_RATE = 0.1\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "EXPLORATION_PROB = 1.0 # EPSILON GREEDY STRATEGY\n",
    "EXPLORATION_DECAY = 0.995\n",
    "EXPLORATION_STOP = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_utility_table():\n",
    "    states = get_all_possible_states()\n",
    "    #utility_values = np.zeros(len(states))\n",
    "    utility_values = np.empty(len(states))\n",
    "    utility_values.fill(100)\n",
    "    utility_table = dict(zip(states, utility_values))\n",
    "    return utility_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reward_table():\n",
    "    states = get_all_possible_states()\n",
    "    rewards = np.zeros(len(states)) # init with zero; high; random\n",
    "    reward_table = dict(zip(states, rewards))\n",
    "    return reward_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_utility_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_number_state_action_table(nsa_table, state, action):\n",
    "#     key = (state, action)\n",
    "#     if key in nsa_table.keys():\n",
    "#         nsa_table[key] += 1\n",
    "#     else:\n",
    "#         nsa_table[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_number_state_action_next_state_table(nsas_table, state, action, next_state):\n",
    "    if (state, action) in nsas_table.keys():\n",
    "        if next_state in nsas_table[(state, action)].keys():\n",
    "            nsas_table[(state, action)][next_state] += 1\n",
    "        else:\n",
    "            nsas_table[(state, action)][next_state] = 1\n",
    "    else:\n",
    "        nsas_table[(state, action)] = {}\n",
    "        nsas_table[(state, action)][next_state] = 1\n",
    "        \n",
    "    return nsas_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_probability(nsas_table, state, action):\n",
    "    next_states = nsas_table[(state, action)]\n",
    "    temp = {}\n",
    "    for next_state in next_states:\n",
    "        temp[next_state] = nsas_table[(state, action)][next_state]/sum(nsas_table[(state, action)].values())\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nsa(nsas_table, state, action):\n",
    "    return sum(nsas_table[(state, action)].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_utility_estimate(utility_table, nsas_table, state, action, next_state, reward_table, epsilon, gamma):\n",
    "    next_states = nsas_table[(state, action)].keys()\n",
    "    u = 0\n",
    "    probs = get_transition_probability(nsas_table, state, action)\n",
    "    for next_state in next_states:\n",
    "        u +=  probs[next_state] * utility_table[next_state] ##\n",
    "    \n",
    "    actions = [0, 1]\n",
    "    f_values = []\n",
    "    if (state, actions[0]) in nsas_table.keys():\n",
    "        f_values.append(exploration_function(u, get_nsa(nsas_table, state, action), epsilon))\n",
    "    if (state, actions[1]) in nsas_table.keys():\n",
    "        f_values.append(exploration_function(u, get_nsa(nsas_table, state, action), epsilon))\n",
    "    if not f_values:\n",
    "        print('(O.O) we have a problem')\n",
    "    \n",
    "    utility_table[state] = reward_table[state] + gamma * max(f_values)\n",
    "    return utility_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_function(utility, n, epsilon):\n",
    "    if n < epsilon:\n",
    "        return 95\n",
    "    else:\n",
    "        return utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(utility_table, nsas_table, state): \n",
    "    best_a0, best_a1 = -100, -100\n",
    "    if (state, 0) in nsas_table.keys():\n",
    "        next_states_a0 = nsas_table[(state, 0)].keys()\n",
    "        best_a0 = max([utility_table[s] for s in next_states_a0])\n",
    "    if (state, 1) in nsas_table.keys():\n",
    "        next_states_a1 = nsas_table[(state, 1)].keys()\n",
    "        best_a1 = max([utility_table[s] for s in next_states_a1])\n",
    "    \n",
    "    if best_a0 == best_a1:\n",
    "        return randint(0,1)\n",
    "    \n",
    "    return (0 if best_a0 > best_a1 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bernhard\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\bernhard\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100 Epsilon: 0.603 Mean-Reward: 77.79 Max-Reward: 498.0\n",
      "Episode: 200 Epsilon: 0.365 Mean-Reward: 104.51 Max-Reward: 175.0\n",
      "Episode: 300 Epsilon: 0.221 Mean-Reward: 107.93 Max-Reward: 205.0\n",
      "Episode: 400 Epsilon: 0.134 Mean-Reward: 110.44 Max-Reward: 211.0\n",
      "Episode: 500 Epsilon: 0.081 Mean-Reward: 106.22 Max-Reward: 184.0\n",
      "Episode: 600 Epsilon: 0.049 Mean-Reward: 109.09 Max-Reward: 184.0\n",
      "Episode: 700 Epsilon: 0.03 Mean-Reward: 108.77 Max-Reward: 209.0\n",
      "Episode: 800 Epsilon: 0.018 Mean-Reward: 104.8 Max-Reward: 236.0\n",
      "Episode: 900 Epsilon: 0.011 Mean-Reward: 112.12 Max-Reward: 399.0\n",
      "Episode: 1000 Epsilon: 0.01 Mean-Reward: 111.5 Max-Reward: 339.0\n",
      "Episode: 1100 Epsilon: 0.01 Mean-Reward: 109.06 Max-Reward: 338.0\n",
      "Episode: 1200 Epsilon: 0.01 Mean-Reward: 113.95 Max-Reward: 222.0\n",
      "Episode: 1300 Epsilon: 0.01 Mean-Reward: 118.2 Max-Reward: 369.0\n",
      "Episode: 1400 Epsilon: 0.01 Mean-Reward: 103.89 Max-Reward: 280.0\n",
      "Episode: 1500 Epsilon: 0.01 Mean-Reward: 105.86 Max-Reward: 292.0\n",
      "Episode: 1600 Epsilon: 0.01 Mean-Reward: 113.3 Max-Reward: 205.0\n",
      "Episode: 1700 Epsilon: 0.01 Mean-Reward: 113.71 Max-Reward: 349.0\n",
      "Episode: 1800 Epsilon: 0.01 Mean-Reward: 105.06 Max-Reward: 336.0\n",
      "Episode: 1900 Epsilon: 0.01 Mean-Reward: 103.83 Max-Reward: 340.0\n",
      "Episode: 2000 Epsilon: 0.01 Mean-Reward: 105.73 Max-Reward: 258.0\n",
      "Episode: 2100 Epsilon: 0.01 Mean-Reward: 109.52 Max-Reward: 261.0\n",
      "Episode: 2200 Epsilon: 0.01 Mean-Reward: 109.67 Max-Reward: 315.0\n",
      "Episode: 2300 Epsilon: 0.01 Mean-Reward: 107.11 Max-Reward: 200.0\n",
      "Episode: 2400 Epsilon: 0.01 Mean-Reward: 104.2 Max-Reward: 242.0\n",
      "Episode: 2500 Epsilon: 0.01 Mean-Reward: 123.48 Max-Reward: 394.0\n",
      "Episode: 2600 Epsilon: 0.01 Mean-Reward: 113.3 Max-Reward: 156.0\n",
      "Episode: 2700 Epsilon: 0.01 Mean-Reward: 114.0 Max-Reward: 144.0\n",
      "Episode: 2800 Epsilon: 0.01 Mean-Reward: 115.54 Max-Reward: 164.0\n",
      "Episode: 2900 Epsilon: 0.01 Mean-Reward: 115.74 Max-Reward: 164.0\n",
      "Episode: 3000 Epsilon: 0.01 Mean-Reward: 112.95 Max-Reward: 148.0\n",
      "Episode: 3100 Epsilon: 0.01 Mean-Reward: 115.56 Max-Reward: 154.0\n",
      "Episode: 3200 Epsilon: 0.01 Mean-Reward: 114.93 Max-Reward: 146.0\n",
      "Episode: 3300 Epsilon: 0.01 Mean-Reward: 117.75 Max-Reward: 161.0\n",
      "Episode: 3400 Epsilon: 0.01 Mean-Reward: 113.7 Max-Reward: 161.0\n",
      "Episode: 3500 Epsilon: 0.01 Mean-Reward: 114.88 Max-Reward: 149.0\n",
      "Episode: 3600 Epsilon: 0.01 Mean-Reward: 117.0 Max-Reward: 150.0\n",
      "Episode: 3700 Epsilon: 0.01 Mean-Reward: 131.07 Max-Reward: 325.0\n",
      "Episode: 3800 Epsilon: 0.01 Mean-Reward: 117.67 Max-Reward: 277.0\n",
      "Episode: 3900 Epsilon: 0.01 Mean-Reward: 112.39 Max-Reward: 316.0\n",
      "Episode: 4000 Epsilon: 0.01 Mean-Reward: 91.1 Max-Reward: 167.0\n",
      "Episode: 4100 Epsilon: 0.01 Mean-Reward: 93.98 Max-Reward: 168.0\n",
      "Episode: 4200 Epsilon: 0.01 Mean-Reward: 97.15 Max-Reward: 197.0\n",
      "Episode: 4300 Epsilon: 0.01 Mean-Reward: 96.26 Max-Reward: 135.0\n",
      "Episode: 4400 Epsilon: 0.01 Mean-Reward: 94.1 Max-Reward: 137.0\n",
      "Episode: 4500 Epsilon: 0.01 Mean-Reward: 94.26 Max-Reward: 184.0\n",
      "Episode: 4600 Epsilon: 0.01 Mean-Reward: 101.09 Max-Reward: 173.0\n",
      "Episode: 4700 Epsilon: 0.01 Mean-Reward: 99.01 Max-Reward: 183.0\n",
      "Episode: 4800 Epsilon: 0.01 Mean-Reward: 98.89 Max-Reward: 188.0\n",
      "Episode: 4900 Epsilon: 0.01 Mean-Reward: 102.74 Max-Reward: 191.0\n",
      "Episode: 5000 Epsilon: 0.01 Mean-Reward: 96.93 Max-Reward: 346.0\n",
      "Episode: 5100 Epsilon: 0.01 Mean-Reward: 84.01 Max-Reward: 131.0\n",
      "Episode: 5200 Epsilon: 0.01 Mean-Reward: 89.2 Max-Reward: 197.0\n",
      "Episode: 5300 Epsilon: 0.01 Mean-Reward: 106.44 Max-Reward: 194.0\n",
      "Episode: 5400 Epsilon: 0.01 Mean-Reward: 107.29 Max-Reward: 132.0\n",
      "Episode: 5500 Epsilon: 0.01 Mean-Reward: 104.71 Max-Reward: 128.0\n",
      "Episode: 5600 Epsilon: 0.01 Mean-Reward: 107.08 Max-Reward: 133.0\n",
      "Episode: 5700 Epsilon: 0.01 Mean-Reward: 108.49 Max-Reward: 146.0\n",
      "Episode: 5800 Epsilon: 0.01 Mean-Reward: 105.14 Max-Reward: 136.0\n",
      "Episode: 5900 Epsilon: 0.01 Mean-Reward: 106.46 Max-Reward: 134.0\n",
      "Episode: 6000 Epsilon: 0.01 Mean-Reward: 108.76 Max-Reward: 213.0\n",
      "Episode: 6100 Epsilon: 0.01 Mean-Reward: 107.74 Max-Reward: 143.0\n",
      "Episode: 6200 Epsilon: 0.01 Mean-Reward: 108.18 Max-Reward: 135.0\n",
      "Episode: 6300 Epsilon: 0.01 Mean-Reward: 92.7 Max-Reward: 189.0\n",
      "Episode: 6400 Epsilon: 0.01 Mean-Reward: 89.14 Max-Reward: 133.0\n",
      "Episode: 6500 Epsilon: 0.01 Mean-Reward: 98.71 Max-Reward: 185.0\n",
      "Episode: 6600 Epsilon: 0.01 Mean-Reward: 96.38 Max-Reward: 211.0\n",
      "Episode: 6700 Epsilon: 0.01 Mean-Reward: 97.7 Max-Reward: 236.0\n",
      "Episode: 6800 Epsilon: 0.01 Mean-Reward: 90.37 Max-Reward: 277.0\n",
      "Episode: 6900 Epsilon: 0.01 Mean-Reward: 97.48 Max-Reward: 125.0\n",
      "Episode: 7000 Epsilon: 0.01 Mean-Reward: 104.65 Max-Reward: 214.0\n",
      "Episode: 7100 Epsilon: 0.01 Mean-Reward: 109.38 Max-Reward: 239.0\n",
      "Episode: 7200 Epsilon: 0.01 Mean-Reward: 108.51 Max-Reward: 135.0\n",
      "Episode: 7300 Epsilon: 0.01 Mean-Reward: 110.14 Max-Reward: 159.0\n",
      "Episode: 7400 Epsilon: 0.01 Mean-Reward: 108.28 Max-Reward: 131.0\n",
      "Episode: 7500 Epsilon: 0.01 Mean-Reward: 90.95 Max-Reward: 133.0\n",
      "Episode: 7600 Epsilon: 0.01 Mean-Reward: 102.13 Max-Reward: 134.0\n",
      "Episode: 7700 Epsilon: 0.01 Mean-Reward: 95.31 Max-Reward: 142.0\n",
      "Episode: 7800 Epsilon: 0.01 Mean-Reward: 94.29 Max-Reward: 159.0\n",
      "Episode: 7900 Epsilon: 0.01 Mean-Reward: 99.59 Max-Reward: 163.0\n",
      "Episode: 8000 Epsilon: 0.01 Mean-Reward: 114.33 Max-Reward: 178.0\n",
      "Episode: 8100 Epsilon: 0.01 Mean-Reward: 96.37 Max-Reward: 188.0\n",
      "Episode: 8200 Epsilon: 0.01 Mean-Reward: 104.21 Max-Reward: 194.0\n",
      "Episode: 8300 Epsilon: 0.01 Mean-Reward: 101.24 Max-Reward: 210.0\n",
      "Episode: 8400 Epsilon: 0.01 Mean-Reward: 96.55 Max-Reward: 198.0\n",
      "Episode: 8500 Epsilon: 0.01 Mean-Reward: 99.64 Max-Reward: 208.0\n",
      "Episode: 8600 Epsilon: 0.01 Mean-Reward: 96.45 Max-Reward: 125.0\n",
      "Episode: 8700 Epsilon: 0.01 Mean-Reward: 95.45 Max-Reward: 126.0\n",
      "Episode: 8800 Epsilon: 0.01 Mean-Reward: 91.4 Max-Reward: 124.0\n",
      "Episode: 8900 Epsilon: 0.01 Mean-Reward: 94.87 Max-Reward: 121.0\n",
      "Episode: 9000 Epsilon: 0.01 Mean-Reward: 96.34 Max-Reward: 126.0\n",
      "Episode: 9100 Epsilon: 0.01 Mean-Reward: 96.2 Max-Reward: 129.0\n",
      "Episode: 9200 Epsilon: 0.01 Mean-Reward: 95.95 Max-Reward: 129.0\n",
      "Episode: 9300 Epsilon: 0.01 Mean-Reward: 94.89 Max-Reward: 121.0\n",
      "Episode: 9400 Epsilon: 0.01 Mean-Reward: 94.77 Max-Reward: 122.0\n",
      "Episode: 9500 Epsilon: 0.01 Mean-Reward: 95.19 Max-Reward: 128.0\n",
      "Episode: 9600 Epsilon: 0.01 Mean-Reward: 95.18 Max-Reward: 135.0\n",
      "Episode: 9700 Epsilon: 0.01 Mean-Reward: 78.52 Max-Reward: 171.0\n",
      "Episode: 9800 Epsilon: 0.01 Mean-Reward: 97.13 Max-Reward: 129.0\n",
      "Episode: 9900 Epsilon: 0.01 Mean-Reward: 91.2 Max-Reward: 118.0\n",
      "Episode: 10000 Epsilon: 0.01 Mean-Reward: 96.19 Max-Reward: 123.0\n",
      "Episode: 10100 Epsilon: 0.01 Mean-Reward: 95.99 Max-Reward: 127.0\n",
      "Episode: 10200 Epsilon: 0.01 Mean-Reward: 94.83 Max-Reward: 128.0\n",
      "Episode: 10300 Epsilon: 0.01 Mean-Reward: 95.38 Max-Reward: 117.0\n",
      "Episode: 10400 Epsilon: 0.01 Mean-Reward: 95.85 Max-Reward: 132.0\n",
      "Episode: 10500 Epsilon: 0.01 Mean-Reward: 92.9 Max-Reward: 122.0\n",
      "Episode: 10600 Epsilon: 0.01 Mean-Reward: 94.58 Max-Reward: 118.0\n",
      "Episode: 10700 Epsilon: 0.01 Mean-Reward: 95.8 Max-Reward: 123.0\n",
      "Episode: 10800 Epsilon: 0.01 Mean-Reward: 97.73 Max-Reward: 164.0\n",
      "Episode: 10900 Epsilon: 0.01 Mean-Reward: 96.63 Max-Reward: 126.0\n",
      "Episode: 11000 Epsilon: 0.01 Mean-Reward: 96.3 Max-Reward: 123.0\n",
      "Episode: 11100 Epsilon: 0.01 Mean-Reward: 96.3 Max-Reward: 120.0\n",
      "Episode: 11200 Epsilon: 0.01 Mean-Reward: 96.7 Max-Reward: 123.0\n",
      "Episode: 11300 Epsilon: 0.01 Mean-Reward: 96.3 Max-Reward: 132.0\n",
      "Episode: 11400 Epsilon: 0.01 Mean-Reward: 95.18 Max-Reward: 125.0\n",
      "Episode: 11500 Epsilon: 0.01 Mean-Reward: 92.43 Max-Reward: 118.0\n",
      "Episode: 11600 Epsilon: 0.01 Mean-Reward: 95.25 Max-Reward: 125.0\n",
      "Episode: 11700 Epsilon: 0.01 Mean-Reward: 93.97 Max-Reward: 150.0\n",
      "Episode: 11800 Epsilon: 0.01 Mean-Reward: 78.28 Max-Reward: 151.0\n",
      "Episode: 11900 Epsilon: 0.01 Mean-Reward: 84.76 Max-Reward: 143.0\n",
      "Episode: 12000 Epsilon: 0.01 Mean-Reward: 97.57 Max-Reward: 127.0\n",
      "Episode: 12100 Epsilon: 0.01 Mean-Reward: 97.02 Max-Reward: 127.0\n",
      "Episode: 12200 Epsilon: 0.01 Mean-Reward: 95.69 Max-Reward: 123.0\n",
      "Episode: 12300 Epsilon: 0.01 Mean-Reward: 84.45 Max-Reward: 115.0\n",
      "Episode: 12400 Epsilon: 0.01 Mean-Reward: 69.22 Max-Reward: 147.0\n",
      "Episode: 12500 Epsilon: 0.01 Mean-Reward: 81.09 Max-Reward: 118.0\n",
      "Episode: 12600 Epsilon: 0.01 Mean-Reward: 94.53 Max-Reward: 125.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 12700 Epsilon: 0.01 Mean-Reward: 78.86 Max-Reward: 122.0\n",
      "Episode: 12800 Epsilon: 0.01 Mean-Reward: 96.41 Max-Reward: 134.0\n",
      "Episode: 12900 Epsilon: 0.01 Mean-Reward: 94.78 Max-Reward: 116.0\n",
      "Episode: 13000 Epsilon: 0.01 Mean-Reward: 94.88 Max-Reward: 121.0\n",
      "Episode: 13100 Epsilon: 0.01 Mean-Reward: 95.29 Max-Reward: 125.0\n",
      "Episode: 13200 Epsilon: 0.01 Mean-Reward: 95.77 Max-Reward: 129.0\n",
      "Episode: 13300 Epsilon: 0.01 Mean-Reward: 95.13 Max-Reward: 199.0\n",
      "Episode: 13400 Epsilon: 0.01 Mean-Reward: 95.74 Max-Reward: 127.0\n",
      "Episode: 13500 Epsilon: 0.01 Mean-Reward: 96.2 Max-Reward: 127.0\n",
      "Episode: 13600 Epsilon: 0.01 Mean-Reward: 94.96 Max-Reward: 121.0\n",
      "Episode: 13700 Epsilon: 0.01 Mean-Reward: 95.32 Max-Reward: 117.0\n",
      "Episode: 13800 Epsilon: 0.01 Mean-Reward: 97.84 Max-Reward: 132.0\n",
      "Episode: 13900 Epsilon: 0.01 Mean-Reward: 96.44 Max-Reward: 132.0\n",
      "Episode: 14000 Epsilon: 0.01 Mean-Reward: 96.21 Max-Reward: 134.0\n",
      "Episode: 14100 Epsilon: 0.01 Mean-Reward: 94.92 Max-Reward: 121.0\n",
      "Episode: 14200 Epsilon: 0.01 Mean-Reward: 94.05 Max-Reward: 122.0\n",
      "Episode: 14300 Epsilon: 0.01 Mean-Reward: 93.21 Max-Reward: 116.0\n",
      "Episode: 14400 Epsilon: 0.01 Mean-Reward: 86.86 Max-Reward: 154.0\n",
      "Episode: 14500 Epsilon: 0.01 Mean-Reward: 94.47 Max-Reward: 122.0\n",
      "Episode: 14600 Epsilon: 0.01 Mean-Reward: 84.07 Max-Reward: 124.0\n",
      "Episode: 14700 Epsilon: 0.01 Mean-Reward: 83.6 Max-Reward: 126.0\n",
      "Episode: 14800 Epsilon: 0.01 Mean-Reward: 82.98 Max-Reward: 122.0\n",
      "Episode: 14900 Epsilon: 0.01 Mean-Reward: 96.5 Max-Reward: 130.0\n",
      "Episode: 15000 Epsilon: 0.01 Mean-Reward: 95.66 Max-Reward: 120.0\n",
      "Episode: 15100 Epsilon: 0.01 Mean-Reward: 94.97 Max-Reward: 129.0\n",
      "Episode: 15200 Epsilon: 0.01 Mean-Reward: 92.49 Max-Reward: 125.0\n",
      "Episode: 15300 Epsilon: 0.01 Mean-Reward: 97.23 Max-Reward: 126.0\n",
      "Episode: 15400 Epsilon: 0.01 Mean-Reward: 95.62 Max-Reward: 121.0\n",
      "Episode: 15500 Epsilon: 0.01 Mean-Reward: 95.48 Max-Reward: 139.0\n",
      "Episode: 15600 Epsilon: 0.01 Mean-Reward: 94.23 Max-Reward: 125.0\n",
      "Episode: 15700 Epsilon: 0.01 Mean-Reward: 91.62 Max-Reward: 132.0\n",
      "Episode: 15800 Epsilon: 0.01 Mean-Reward: 96.01 Max-Reward: 125.0\n",
      "Episode: 15900 Epsilon: 0.01 Mean-Reward: 81.41 Max-Reward: 115.0\n",
      "Episode: 16000 Epsilon: 0.01 Mean-Reward: 86.96 Max-Reward: 131.0\n",
      "Episode: 16100 Epsilon: 0.01 Mean-Reward: 91.99 Max-Reward: 119.0\n",
      "Episode: 16200 Epsilon: 0.01 Mean-Reward: 76.07 Max-Reward: 151.0\n",
      "Episode: 16300 Epsilon: 0.01 Mean-Reward: 64.49 Max-Reward: 125.0\n",
      "Episode: 16400 Epsilon: 0.01 Mean-Reward: 87.03 Max-Reward: 123.0\n",
      "Episode: 16500 Epsilon: 0.01 Mean-Reward: 95.49 Max-Reward: 121.0\n",
      "Episode: 16600 Epsilon: 0.01 Mean-Reward: 96.3 Max-Reward: 127.0\n",
      "Episode: 16700 Epsilon: 0.01 Mean-Reward: 94.98 Max-Reward: 118.0\n",
      "Episode: 16800 Epsilon: 0.01 Mean-Reward: 89.26 Max-Reward: 120.0\n",
      "Episode: 16900 Epsilon: 0.01 Mean-Reward: 95.3 Max-Reward: 121.0\n",
      "Episode: 17000 Epsilon: 0.01 Mean-Reward: 90.41 Max-Reward: 116.0\n",
      "Episode: 17100 Epsilon: 0.01 Mean-Reward: 95.13 Max-Reward: 119.0\n",
      "Episode: 17200 Epsilon: 0.01 Mean-Reward: 96.18 Max-Reward: 137.0\n",
      "Episode: 17300 Epsilon: 0.01 Mean-Reward: 79.76 Max-Reward: 140.0\n",
      "Episode: 17400 Epsilon: 0.01 Mean-Reward: 72.54 Max-Reward: 176.0\n",
      "Episode: 17500 Epsilon: 0.01 Mean-Reward: 85.39 Max-Reward: 186.0\n",
      "Episode: 17600 Epsilon: 0.01 Mean-Reward: 92.3 Max-Reward: 152.0\n",
      "Episode: 17700 Epsilon: 0.01 Mean-Reward: 95.75 Max-Reward: 122.0\n",
      "Episode: 17800 Epsilon: 0.01 Mean-Reward: 94.6 Max-Reward: 118.0\n",
      "Episode: 17900 Epsilon: 0.01 Mean-Reward: 97.53 Max-Reward: 194.0\n",
      "Episode: 18000 Epsilon: 0.01 Mean-Reward: 95.17 Max-Reward: 125.0\n",
      "Episode: 18100 Epsilon: 0.01 Mean-Reward: 95.39 Max-Reward: 123.0\n",
      "Episode: 18200 Epsilon: 0.01 Mean-Reward: 94.94 Max-Reward: 120.0\n",
      "Episode: 18300 Epsilon: 0.01 Mean-Reward: 95.15 Max-Reward: 123.0\n",
      "Episode: 18400 Epsilon: 0.01 Mean-Reward: 95.47 Max-Reward: 127.0\n",
      "Episode: 18500 Epsilon: 0.01 Mean-Reward: 96.45 Max-Reward: 125.0\n",
      "Episode: 18600 Epsilon: 0.01 Mean-Reward: 97.12 Max-Reward: 123.0\n",
      "Episode: 18700 Epsilon: 0.01 Mean-Reward: 94.3 Max-Reward: 114.0\n",
      "Episode: 18800 Epsilon: 0.01 Mean-Reward: 95.99 Max-Reward: 152.0\n",
      "Episode: 18900 Epsilon: 0.01 Mean-Reward: 96.32 Max-Reward: 122.0\n",
      "Episode: 19000 Epsilon: 0.01 Mean-Reward: 96.62 Max-Reward: 175.0\n",
      "Episode: 19100 Epsilon: 0.01 Mean-Reward: 96.28 Max-Reward: 117.0\n",
      "Episode: 19200 Epsilon: 0.01 Mean-Reward: 98.18 Max-Reward: 123.0\n",
      "Episode: 19300 Epsilon: 0.01 Mean-Reward: 87.74 Max-Reward: 119.0\n",
      "Episode: 19400 Epsilon: 0.01 Mean-Reward: 96.09 Max-Reward: 179.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-d930e1142fd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation_to_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m             \u001b[0moverall_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-63f1090af0f2>\u001b[0m in \u001b[0;36mobservation_to_state\u001b[1;34m(observation)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mobservation_to_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdiscrete_observation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscretize_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdigits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTATES_IN_INTERVAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-33491ef29deb>\u001b[0m in \u001b[0;36mdiscretize_observation\u001b[1;34m(observation)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdiscretize_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdiscrete_observation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigitize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mINTERVALS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOBSERVATION_SPACE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m# if some value is under the lower border ignore it and give it min value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdiscrete_observation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdiscrete_observation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdiscrete_observation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-33491ef29deb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdiscretize_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdiscrete_observation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigitize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mINTERVALS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOBSERVATION_SPACE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m# if some value is under the lower border ignore it and give it min value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdiscrete_observation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdiscrete_observation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdiscrete_observation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mdigitize\u001b[1;34m(x, bins, right)\u001b[0m\n\u001b[0;32m   4807\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mside\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4808\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4809\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mside\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msearchsorted\u001b[1;34m(a, v, side, sorter)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m     \"\"\"\n\u001b[1;32m-> 1246\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'searchsorted'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mside\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mside\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msorter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "INTERVALS = create_state_intervals()\n",
    "number_of_games = 200000\n",
    "\n",
    "\n",
    "#data = pd.DataFrame(columns = ['mean', 'max', 'solve_step'])\n",
    "data_max = []\n",
    "data_mean = []\n",
    "data_solve = []\n",
    "\n",
    "solved = False\n",
    "\n",
    "for i in range(1):\n",
    "    print('EPISODE:', i)\n",
    "        # init things we dont understand\n",
    "    utility_table = create_utility_table()\n",
    "    nsas_table = {}\n",
    "    reward_table = create_reward_table()\n",
    "\n",
    "    gamma = 0.9\n",
    "    explore = 1 ## change this (jahrers idea ... critical)\n",
    "    epsilon = EXPLORATION_PROB\n",
    "    \n",
    "    if i != 0:\n",
    "        data_max.append(game_max)\n",
    "        data_mean.append(game_mean)\n",
    "        if not solved:\n",
    "            data_solve.append(-1)\n",
    "    last100_rewards = deque(maxlen=100) # fifo queue\n",
    "    game_max = []\n",
    "    game_mean = []\n",
    "    solved = False\n",
    "    for game in range(number_of_games):\n",
    "        overall_reward, done = 0, False\n",
    "        observation = env.reset()\n",
    "        state = observation_to_state(observation)\n",
    "\n",
    "        while not done:\n",
    "            #if game % 1000 == 0: env.render()\n",
    "            #if np.random.rand() < epsilon:\n",
    "            #    action = env.action_space.sample()\n",
    "            #else:\n",
    "            action = get_action(utility_table, nsas_table, state)\n",
    "\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            next_state = observation_to_state(observation)\n",
    "            overall_reward += reward\n",
    "\n",
    "            if done: reward = -500 # punish if agent dies\n",
    "\n",
    "            reward_table[state] = reward\n",
    "            nsas_table = update_number_state_action_next_state_table(nsas_table, state, action, next_state)\n",
    "\n",
    "            utility_table = update_utility_estimate(utility_table, nsas_table, state, action, next_state, reward_table, explore, gamma)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        if epsilon > EXPLORATION_STOP: epsilon *= EXPLORATION_DECAY\n",
    "\n",
    "        if game % 100 == 0 and game != 0:\n",
    "            print('Episode:', game, 'Epsilon:', round(epsilon, 3), \n",
    "                  'Mean-Reward:', np.mean(last100_rewards), 'Max-Reward:', max(last100_rewards))\n",
    "        if (np.mean(last100_rewards) >= 195) and not solved: \n",
    "            print('TASK COMPLETED LAST 100 GAMES HAD AN AVERAGE SCORE >=195 ON GAME', game)\n",
    "            print(last100_rewards)\n",
    "            solved = True\n",
    "            data_solve.append(game)\n",
    "            \n",
    "        if game % 100 == 0 and game != 0:\n",
    "            game_max.append(max(last100_rewards))\n",
    "            game_mean.append(np.mean(last100_rewards))\n",
    "        \n",
    "        last100_rewards.append(overall_reward) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
